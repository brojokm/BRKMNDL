<html>
<body>
Today we are going to introduce graph search in general and talk about one algorithm, which is breadth first search, and understand how in principle you can solve a puzzle like the Rubik is Cube. So before I get to Rubik is Cubes let me remind you of some basic stuff about graphs. Or I can tell you to start out with, graph search is about exploring a graph. And there is many different notions of exploring a graph. Maybe I give you some node in a graph, s, and some other node in a graph, t, and I had like to find a path that is going to represent a problem like I give you a particular state of a Rubik is Cube and I want to know is there some path that gets me into a solved state? Do I really want to solve this on stage? What the hell? We started. So this is a particularly easy state to solve, which is why I set up this way. All right, so there you go. Seven by seven by seven Rubik is Cube solved in ten seconds. Amazing. New world record. So you are given some initial state of the Rubik is Cube. You are given the targets that you know what solved looks like. You want to find this path. Maybe you want to find all paths from s. Maybe you just want to explore all the nodes in a graph you can reach from s. Maybe you want to explore all the nodes in a graph or maybe all the edges in a graph. These are all exploration problems. They are all going to be solved by algorithms from this class and next class. So before we go further though, I should remind you what a graph is and sort of basic features of graphs that we are going to be using. This is also six thousand and forty two material so you should know it very well. If you do not, there is an appendix in the textbook about it. We have a set of vertices. We have a set of edges. Edges are either unordered pairs some sets of two items or ordered pairs. In this case, we call the graph undirected. In this case, we call the graph directed. Usually, there is only one type. Either all the edges are directed or all the edges are undirected. There is a study of graphs that have both, but we are not doing that here. Some simple examples. Here is a graph. This is an undirected graph. This is a directed graph. The set of vertices here is a, b, c, d. The set of vertices here is a, b, c. The set of edges here is E is going to be things like a, b b, c c, d I think you get the idea. Just for completeness, V is a, b, c, d. Just so you remember notations and so on. One of the issues we are going to talk about in this class is how do you represent a graph like this for an algorithm? So it is all fine to say, oh, this is a set of things. This is a set of things. An obvious representation is, you have a list or an array of vertices. You have an array of edges. Each edge knows its two end points. That would be a horrible representation for a graph because if you are, I do not know, at vertex, a, and you want to know, well what are the neighbors of a? b and c. You had have to go through the entire edge list to figure out the neighbors of a. So it is been linear time just to know where you can go from a. So we are not going to use that representation. We are going to use some better representations. Something called an adjacency list. Over here, you have got things like a, c b, c and c, b. So you can have edges in both directions. What am I missing? b, a. So that is E, in that case. There are a whole lot of applications of graph search. I will make you a little list to talk about few of them. So we have got web crawling. You are Google. You want to find all the pages on the web. Most people do not just tell you, hey, I have got a new page, please index it. You have to just keep following links in the early days of the web, this was a big deal following links finding everything that is out there. It is a little bit of an issue because if you define it wrong, the internet is infinite because of all those dynamically generated pages. But to deal with that, Google goes sort of breadth first for the most part. It is prioritized You want to see all the things you can reach from pages you already have and keep going. At some point, you give up when you run out of time. Social networking. You are on Facebook. You use Friend Finder. It tries to find the friends that are nearest to you. Or friends of friends is sort of a level to search. That is essentially a graph search problem. You want to know what is two levels or three levels of separation from you. And then you loop over those and look for other signs that you might be good friends. You are on a network like the internet or some intranet. You want to broadcast a message. So here is you. You want to send data out. That is essentially a graph exploration problem. That message, that packet, is going to explore the graph. Garbage collection. I hope you all know that modern languages have garbage collection. This is why you do not have to worry about freeing things. Even in Python even in CPython, I learned there is a garbage collector as of version two. But also in PyPy, and JPython and in Java pretty much every fairly modern language you have garbage collection. Meaning, if there is some data that is unreachable from So you have your variables. Variables that can be accessed by the program. Everything that is reachable from there you have to keep. But if some data structure becomes no longer reachable, you can throw it away and regain memory. So that is happening behind the scenes all the time, and the way it is being done is with their breadth first search, which is what we are going to talk about today. Another one. Model checking. Model checking is you have some finite model of either a piece of code, or a circuit, or chip, whatever, and you want to prove that it actually does what you think it does. And so you have drawn a graph. The graph is all the possible states that your circuit or your computer program could reach, or that it could possibly have. You start in some initial state, and you want to know among all the states that you can reach, does it have some property. And so you need to visit all the vertices that are reachable from a particular place. And usually people do that using breadth first search. I use breadth first search a lot, myself, to check mathematical conjectures. So if you are a mathematician, and you think something is true. Like maybe It is hard to give an example of that. But you can imagine some graph of all the possible inputs to that theorem, and you need to check them for every possible input If this is true the typical way to do that is breadth first searching through that entire graph of states. Usually, we are testing finite, special cases of a general conjecture, but if we find a counter example, we are done. Do not have to work on it anymore. If we do not find a counter example, usually then we have to do the mathematics. It does not solve everything, but it is helpful. And then, the fun thing we are going to talk about a little bit today, is if you want to solve something like a two by two by two Rubik is Cube optimally, you can do that using breadth first search. And you are going to do that on your problem set. To do it solving this one optimally using breadth first search would probably would definitely take more than the lifetime of the universe. So do not try seven by seven by seven. Leave that to the cubing experts, I guess. I think no one will ever solve a seven by seven by seven Rubik is Cube optimally. There are ways to find a solution just not the best one. So let me tell you just for fun, as an example. This Pocket Cube, which is a two by two by two Rubik is Cube. What we have in mind is called the configuration graph or sometimes configuration space. But it is a graph, so we will call it a graph. This graph has a vertex for each possible state of the cube. So this is a state. This is a state. This is a state. This is a state. Now I am hopelessly lost. Anyone want to work on this? Bored? No one? Alright, I will leave it unsolved then. So all those are vertices. There is actually a lot of vertices. There are two hundred and sixty four million vertices or so. If you want. To the side here. Number of vertices is something like eight factorial times three to the eight point And one way to see that is to draw a two by two by two Rubik is Cube. So these are what you might call cubelets, or cubies I think is the standard term in Rubik is Cube land. There is eight of them in a two by two by two. Two cubed. You can essentially permute those cubies within the cube however you like. That is eight factorial. And then each of them has three possible twists. It could be like this. It could be like this. Or it could be like this. So you have got three for each. And this is actually an accurate count. You are not over counting the number of configurations. All of those are, at least in principle, conceivable. If you take apart the cube, you can reassemble it in each of those states. And that number is about two hundred and sixty four million. Which is not so bad for computers. You could search that. Life is a little bit easier. You get to divide by twenty four because there is twenty four symmetries of the cube. Eight times three. You can divide by three, also, because only a third of the configuration space is actually reachable. If you are not allowed to take the parts apart, if you have to get there by a motion, you can only get to 13 of the two by two by two. So it is a little bit smaller than that, if you are actually doing a breadth first search, which is what you are going to be doing on your problem set. But in any case, it is feasible. That was vertices. We should talk about edges. For every move every move takes you from one configuration to another. You could traverse it in one direction and make that move. You could also undo that move. Because every move is undoable in a Rubik is Cube, this graph is undirected. Or you can think of it as every edge works in both directions. So this is a move. It is called a quarter twist. This is a controversy if you will. Some people allow a whole half twist as a single move. Whether you define that as a single move or a double move is not that big a deal. It just changes some of the answers. But you are still exploring essentially the same graph. So that is the graph and you had like to know some properties about it. So let me draw a picture of the graph. I am not going to draw all two hundred and sixty four million vertices. But in particular, there is the solved state we kind of care about that one, where all the colors are aligned then there is all of the configurations you could reach by one move. So these are the possible moves from the solved state. And then from those configurations, there is more places you can go. Maybe there is multiple ways to get to the same node. But these would be all the configurations you can reach in two moves. And so on. And at some point, you run out of graph. So there might be a few nodes out here. The way I am drawing this, this is everything you can reach in one move, in two movies, in three moves. At the end, this would be eleven moves, if you allow half twists. And as puzzlers, we are particularly interested in this number, which you would call, as a graph theorist, the diameter of the graph. Puzzlers call it God is number. If you were God or some omni something being. You have the optimal algorithm for solving the Rubik is Cube. How many moves do you need If you always follow the best path? And the answer is, in the worst case, eleven point So we are interested in the worst case of the best algorithm. For two by two by two, the answer is eleven point For three by three by three, the answer is twenty point That was just proved last summer with a couple years of computer time. For four by four by four I do not have one here I think we will never know the answer. For five by five by five, we will never know the answer. For six, for seven, same deal. But for two by two by two, you can compute it. You will compute it on your problem set. And it is kind of nice to know because it says whatever configuration I am in, I can solve it in eleven moves. But the best known way to compute it, is basically to construct this graph one layer at a time until you are done. And then you know what the diameter is. The trouble is, in between here this grows exponentially. At some point, it decreases a little bit. But getting over that exponential hump is really hard. And for three by three by three, they used a lot of tricks to speed up the algorithm, but in the end it is essentially a breadth first search. What is a breadth first search? This going layer by layer. So we are going to formalize that in a moment. But that is the problem. So just for fun, any guesses what the right answer is for an n by n by n Rubik is cube? What is the diameter? Not an exact answer, because I think we will never know the exact answer. But if I want theta something, what do you think the something is? How many people here have solved the Rubik is Cube? Ever? So you know what we are talking about here. Most people have worked on it. To think about an n by n by n Rubik is Cube, each side has area n squared. So total surface area is six n squared. So there is, roughly, stata n squared little cubies here. So what do you think the right is for n by n by n? No guesses? n cubed? n cubed? Reasonable guess. But wrong. It is an upper bounds. Why n cubed? . Oh, you are guessing based on the numbers. Yeah. The numbers are misleading, unfortunately. It is the law of small numbers I guess. It does not really look right. I know the answer. I know the answer because we just wrote a paper with the answer. This is a new result. From this summer. But I am curious. To me the obvious answer is n squared because there is about n squared cubies. And it is not so hard to show in a constant number moves you can solve a constant number of cubies. If you think about the general algorithms, like if you have ever looked up professor is cube and how to solve it, you are doing like ten moves, and then maybe you swap two cubies which you can use to solve a couple of cubies in a constant number of moves. So n squared would be the standard answer if you are following standard algorithms. But it turns out, you can do a little bit better. And the right answer is n squared divided by log n. I think it is cool. Hopefully, you guys can appreciate that. Not a lot of people can appreciate n squared divided by log n, but here in algorithms, we are all about n squared over log n. If you are interested, the paper is on my website. I think its called, Algorithms For Solving Rubik is Cubes. There is a constant there. Current constant is not so good. Let is say it is in the millions.  You have got to start somewhere. The next open problem will be to improve that constant to something reasonable that maybe is close to twenty point But we are far from that. Let is talk about graph representation. Before we can talk about exporting a graph, we need to know what we are given as input. And there is basically one standard representation and a bunch of variations of it. And they are called adjacency lists. So the idea with an adjacency list, is you have an array called Adj, for adjacency of size V. Each element in the array is a pointer to a linked list. And the idea is that this array is indexed by a vertex. So we are imagining a world where we can index arrays by vertices. So maybe, you just label your vertices zero through v minus one point Then that is a regular array. Or, if you want to get fancy, you can think of a vertex as an arbitrary hashable thing, and Adj is actually a hash table. And that is how you probably do it in Python. Maybe your vertices are objects, and this is just hashing based on the address of the object. But we are not going to worry about that. We are just going to write Adj of u. Assume that somehow you can get to the linked list corresponding to that vertex. And the idea is, for every vertex we just store its neighbors, namely the vertices you can reach by one step from u. So I am going to define that a little more formally. Adj of u is going to be the set of all vertices, V, such that u, v is an edge. So if I have a vertex like b, Adj of b is going to be both a and c because in one step there are outgoing edges from b to a and b to c. So Adj of b is a, c. In that graph. I should have labeled the vertices something different. Adj of a is going to be just c because you ca not get with one step from a to b. The edge is in the wrong direction. And Adj of c is b. I think that definition is pretty clear. For undirected graphs, you just put braces here. Which means you store I mean, it is the same thing. Here Adj of c is going to be a, b, and d, as you can get in one step from c to a, from c to b, from c to d. For pretty much every At least for graph exploration problems, this is the representation you want. Because you are at some vertex, and you want to know, where can I go next. And Adj of that vertex tells you exactly where you can go next. So this is what you want. There is a lot of different ways to actually implement adjacency lists. I have talked about two of them. You could have the vertices labeled zero to v minus 1, and then this is, literally, an array. And you have I guess I should draw. In this picture, Adj is an array. So you have got a, b, and c. Each one of them is a pointer to a linked list. This one is actually going to be a, c, and we are done. Sorry, that was b. Who said it had to be alphabetical order? A is a pointer to c, c is a pointer to b. That is explicitly how you might represent it. This might be a hash table instead of an array, if you have weirder vertices. You can also do it in a more object oriented fashion. For every vertex, v, you can make the vertices objects, and v dot neighbors could store what we are defining over there to be Adj of v. This would be the more object oriented way to do it I have thought a lot about this, and I like this, and usually when I implement graphs this is what I do. But it is actually convenient to have this representation. There is a reason the textbook uses this representation. Because, if you have already got some vertices lying around and you want to have multiple graphs on those vertices, this lets you do that. You can define multiple Adj arrays, one for graph one, one for graph two, one for graph three but they can all talk about the same vertices. Whereas here, vertex can only belong to one graph. It can only have one neighbor structure that says what happens. If you are only dealing with one graph, this is probably cleaner. But with multiple graphs, which will happen even in this class, adjacency lists are kind of the way to go. You can also do implicitly represented graphs. Which would be to say, Adj of u is a function. Or v dot neighbors is a method of the vertex class. Meaning, it is not just stored there explicitly. Whenever you need it, you call this function and it computes what you want. This is useful because it uses less space. You could say this uses zero space or maybe v space. One for each vertex. It depends. Maybe you do not even need to explicitly represent all the vertices. You start with some vertex, and given a vertex, somehow you know how to compute, let is say in constant time or linear time or something, the neighbors of that vertex. And then from there, you can keep searching, keep computing neighbors, until you find what you want. Maybe you do not have to build the whole graph, you just need to build enough of it until you find your answer. Whatever answer you are searching for. Can you think of a situation where that might be the case? Where implicit representation would be a good idea? Yes. Rubik is Cubes. They are really good. I never want to build this space. It has a bajillion states. A bajillion vertices. It would take forever. There is more configurations of this cube than there are particles in the known universe. I just computed that in my head.  I have done this computation recently, and for five by five by five it is like ten to the forty states. Or ten to the 40, ten to the sixty point There is about ten to the eighty particles in the known universe. ten to the eighty three or something. So this is probably ten to the two hundred or so. It is a lot. You never want to build that. But, it is very easy to represent this state. Just store where all the cubies are. And it is very easy to see what are all the configurations you can reach in one move. Just try this move, try this move, try this move. Put it back and try the next move. And so on. For an m by n by n cube in order n time, you can list all the order n next states. You can list all the order n neighbors. And so you can keep exploring, searching for your state. Now you do not want to explore too far for that cube, but at least you are not hosed just from the problem of representing the graph. So even for two by two by two, it is useful to do this mostly to save space. You are not really saving time. But you had like to not have to store all two hundred and sixty four million states because it is going to be several gigabytes and it is annoying. Speaking of space ignoring the implicit representation how much space does this representation require? V plus E. This Is going to be the bread and butter of our graph algorithms. Most of the things we are going to talk about achieve V plus E time. This is essentially optimal. It is linear in the size of your graph. You have got V vertices, E edges. Technically, in case you are curious, this is really the size of V plus the size of E. But in the textbook, and I guess in the world, we just omit those sizes of whenever they are in a theta notation or Big O notation. So number vertices plus number of edges. that sort of the bare minimum you need if you want an explicit representation of the graph. And we achieve that because we have got we have got v space just to store the vertices in an array. And then if you add up Each of these is an edge. You have to be a little careful. In undirected graphs, each of these is a half edge. So there is actually two times e nodes over here. But it is theta E. So theta V plus E is the amount of space we need. And ideally, all our algorithms will run in this much time. Because that is what you need just to look at the graph. So let is do an actual algorithm, which is breadth first search. So to the simplest algorithm you can think of in graphs. I have already outlined it several times. You start at some node. You look at all the nodes you can get to from there. You look at all the nodes you can get to from there. Keep going until you are done. So this is going to explore all of the vertices that are reachable from a node. The challenge The one annoying thing about breadth first search and why this is not trivial is that there can be some edges that go sort of backwards, like that, to some previous layer. Actually, that is not true, is it? This ca not happen. You see why? Because if that edge existed, then from this node you had be able to get here. So in an undirected graph, that ca not happen. In a directed graph, you could conceivably have a back edge like that. You had have to realize, oh, that is a vertex I have already seen, I do not want to put it here, even though it is something I can reach from this node, because I have already been there. We have got to worry about things like that. That is, I guess, the main thing to worry about. So our goal is to visit all the nodes the vertices reachable from given node, s. We want to achieve V plus E time. And the idea is to look at the nodes that are reachable first in zero moves. Zero moves. That is s. Then in one move. Well that is everything you can reach from s in one step. That is adjacency of s. And then two moves, and three moves, and so on until we run out of graph. But we need to be careful to avoid duplicates. We want to avoid revisiting vertices for a couple of reasons. One is if we did not, we would spend infinite time. Because we had just go there and come back, and go there and come back. As long as there is at least one cycle, you are going to keep going around the cycle forever and ever if you do not try to avoid duplicates. So let me write down some code for this algorithm. It is pretty straightforward. So straightforward, we can be completely explicit and write code. There is a few different ways to implement this algorithm. I will show you my favorite. The textbook has a different favorite. I am going to write in pure Python, I believe. Almost done. I think I got that right. So this is at the end of the while loop. And at that point we should be done. We can do an actual example, maybe. I am going to do it on an undirected graph, but this algorithm works just as well on directed and undirected graphs. There is an undirected graph. We are given some start vertex, s, and we are given the graph by being given the adjacency lists. So you could iterate over the vertices of that thing. Given a vertex, you can list all the edges you can reach in one step. And then the top of the algorithm is just some initialization. The basic structure We have this thing called the frontier, which is what we just reached on the previous level. I think that is going to be level i minus one. Just do not want to make an index error. These are going to be all the things you can reach using exactly i minus one moves. And then next is going to be all the things you can reach in i moves. So to get started, what we know is s. s is what you can reach in zero moves. So we set the level of s to be zero. That is the first line of the code. There is this other thing called the parent. We will worry about that later. It is optional. It gives us some other fun structure. We set i to be one because we just finished level zero. Frontier of what you can reach in level zero is just s itself. So we are going to put that on the list. That is level zero. i equals one So one minus one is zero. All good. And then we are going to iterate. And this is going to be looking at The end of the iteration is to increment i. So you could also call this a for loop except we do not know when it is going to end. So it is easier to think of i incrementing each step not knowing when we are going to stop. We are going to stop whenever we run out of nodes. So whenever frontier is a non empty list. the bulk of the work here is computing what the next level is. That is called next. It is going to be level i. We do some computation. Eventually we have what is on the next level. Then we set frontier next. Because that is our new level. We increment i, and then invariant of frontier being level i minus one is preserved. Right after here. And then we just keep going till we run out of nodes. How do we compute next? Well, we look at every node in the frontier, and we look at all the nodes you can reach from those nodes. So every node, u, in the frontier and then we look at So this means there is an edge from u to v through the picture. We look at all the edges from all the frontier nodes where you can go. And then the key thing is we check for duplicates. We see, have we seen this node before? If we have, we would have set it is level to be something. If we have not seen it, it will not be in the level hash table or the level dictionary. And so if it is not in there, we will put it in there and add it to the next layer. So that is how you avoid duplicates. You set its level to make sure you will never visit it again, you add it to the next frontier, you iterate, you are done. This is one version of what you might call a breadth first search. And it achieves this goal, visiting all the nodes reachable from s, in linear time. Let is see how it works on a real example. So first frontier is this thing. Frontier just has the node s, so we just look at s, and we look at all the edges from s. We get a and x. So those get added to the next frontier. Maybe before I go too far, let me switch colors. Multimedia here. So here is level one. All of these guys, we are going to set their level to one. They can be reached in one step. That is pretty clear. So now frontier is a and x. That is what next becomes. Then frontier becomes next. And so we look at all the edges from a. That is going to be s and z. s, we have already looked at, it already has a level set, so we ignore that. So we look at z. Z does not have a level indicated here, so we are going to set it to i which happens to be two at this point. And we look at x. It has neighbors s, d, and c. We look at s again. We say, oh, we have already seen that yet again. So we are worried about this taking a lot of time because we look at s three times in total. Then we look at d. d has not been set, so we set it to two. c has not been set, so we set it to two. So the frontier at level two is that. Then we look at all the neighbors of z. There is a. a is already been set. Look at all the neighbors of d. There is x. There is c. Those have been set. There is f. This one gets added. Then we look at c. There is x. That is been done. d is been done. f is been done. v has not been done. So this becomes a frontier at level three. Then we look at level three. There is f. D is been done, c is been done, b is been done. We look at v. c is been done. f is been done. Nothing to add to next. Next becomes empty. Frontier becomes empty. The while loop finishes. TA DA We have computed we have visited all the vertices. Question. . What notation? This is Python notation. You may have heard of Python. This is a dictionary which has one key value, s, and has one value, zero. So you could That is shorthand in Python for Usually you have a comma separated list. The colon is specifying key value pairs. I did not talk about parent. We can do that for a little bit. So parent we are initializing to say, the parent of s is nobody, and then whenever we visit a new vertex, v, we set its parent to be the vertex that we came from. So we had this vertex, v. We had an edge to v from some vertex, u. We set the parent of v to be u. So let me add in what that becomes. I will change colors yet again. Although it gets hard to see any color but red. So we have s. When we visited a, then the parent of a would become s. When we visited z, the parent of z would be a. Parent of x is going to be s. Parent of d is going to be x. The parent of c is going to be x. The parent of f it could have been either way, but the way I did it, d went first, and so that became its parent. And I think for v, c was its parent. So that is what the parent pointers will look like. They always follow edges. They actually follow edges backwards. If this was a directed graph, the graph might be directed that way but the parent pointers go back along the edges. So it is a way to return. It is a way to return to s. If you follow these pointers, all roads lead to s. Because we started at s, that is the property we have. In fact, these pointers always form a tree, and the root of the tree is s. In fact, these pointers form what are called shortest paths. Let me write down a little bit about this. Shortest path properties. If you take a node, and you take its parent, and you take the parent of the parent, and so on, eventually you get to s. And if you read it backwards, that will actually be a path in the graph. And it will be a shortest path, in the graph, from s to v. Meaning, if you look at all paths in the graph that go from s to v So say we are going from s to v, how about that, we compute this path out of BFS. Which is, follow a parent of v is c, parent of c is x, parent of x is s. Read it backwards. That gives us a path from s to v. The claim is, that is the shortest way to get from s to v. It might not be the only one. Like if you are going from s to f, there is two short paths. There is this one of length three. There is this one of length three.. Uses three edges. Same length. And in the parent pointers, we can only afford to encode one of those paths because in general there might be exponentially many ways to get from one node to another. We find a shortest path, not necessarily the only one. And the length of that path So shortest here means that you use the fewest edges. And the length will be level of v. That is what we are keeping track of. If the level is zero, you can get there with zero steps. If the level is one, you get there with one steps. Because we are visiting everything you can possibly get in k steps, the level is telling you what that shortest path distance is. And the parent pointers are actually giving you the shortest path. That is the cool thing about BFS. Yeah, BFS explores the vertices. Sometimes, that is all you care about. But in some sense, what really matters, is it finds the shortest way to get from anywhere to anywhere. For a Rubik is Cube, that is nice because you run BFS from the start state of the Rubik is Cube. Then you say, oh, I am in this state. You look up this state. You look at its level. It says, oh, you can get there in nine steps. That is, I think, the average. So I am guessing. I do not know how to do this in nine steps. Great, so now you know how to solve it. You just look at the parent pointer. The parent pointer gives you another configuration. You say, oh, what move was that? And then you do that move. I am not going to solve it. Then you look at the parent pointer of that. You do that move. You look at the parent pointer of that. You do that move. Eventually, you will get to the solved state, and you will do it using the fewest possible moves. So if you can afford to put the whole graph in memory, which you ca not for a big Rubik is Cube but you can for a small one, then this will give you a strategy, the optimal strategy, God is algorithm if you will, for every configuration. It solves all of them. Which is great. What is the running time of this algorithm? I claim it is order V plus E. But it looked a little wasteful because it was checking vertices over and over and over. But if you think about it carefully, you are only looking what is the right way to say this you only check every edge once. Or in undirected graphs, you check them twice, once from each side. A vertex enters the frontier only once. Because once it is in the frontier, it gets a level set. And once it has a level set, it will never go in again. It will never get added to next. So s gets added once then we check all the neighbors of s. a gets added once, then we check all the neighbors of a. Each of these guys gets added once. We check all the neighbors. So the total running time is going to be the sum over all vertices of the size of the adjacency list of v. So this is the number of neighbors that v has. And this is going to be? Answer? Two times the number of edges. Sorry Double the number of edges. Twice the number of edges for undirected graphs. It is going to be the number of edges for directed graphs. This is the Handshaking Lemma. If you do not remember the Handshaking Lemma, you should read the textbook. Six o four two stuff. Basically you visit every edge twice. For directed graphs, you visit every edge once. But it is order E. We also spend order V because we touch every vertex. So the total running time is order V plus E. In fact, the way this is going, you can be a little tighter and say it is order E. I just want to mention in reality Sometimes you do not care about just what you can reach from s, you really want to visit every vertex. Then you need another outer loop that is iterating over all the vertices as potential choices for s. And you then can visit all the vertices in the entire graph even if it is disconnected. We will talk more about that next class. That is it for BFS. 
</body>
</html>