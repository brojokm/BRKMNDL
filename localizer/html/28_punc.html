<html>
<body>
If you guys want me to cover anything in particular, is there anything you did not understand in lecture? In the last section, I covered the recursion trees because they will be on the Pset, and people said they were a bit unclear, so we can do that and cover less of the stuff that I have here. Or if there is anything else, you can tell me what you want. So there I cover recursion trees because someone said, hey, can you go over that again? Is there any pain points? No? OK. So then I am going to give you the same choice that I gave to people last time, and that is we can go over recursion trees again, but if I do that, then I wo not have time to go over the code for deleting a node from a binary search tree. So we will go through the theory and you guys will have to go through the code on your own. But instead, we will go over recursion trees again and remember how you solve a recurrence using recursion trees. The alternative is we do not do that and we complete the deletions part. I feel like covering deletions, since we did not do that in lecture, that would probably be more helpful. Let is take a vote. Who wants to do deletions in painstaking detail? So deletions and not recursion? Who wants to do recursion trees and not deletion? It is about equal. It is equal and nobody cares. I am really sad. Let is do both in half detail. OK, sure. Who remembers merge sort? What does merge sort do really quick? It takes some sort of unsorted array, splits it in half, and then continually splits it, and then once it finally gets to the point where you have arrays of two elements, then it sorts them, and then sorts those, and then sorts those. It is a fun thing. And then it merges. That is so much code. I do not like to write much code because for every line of code that you write, you might have a bug in it, so I like to write less code. So the way I do it is when I get to an array size of one element, I know it is already sorted. So merge sort. You have an array, it is unsorted. Split it into two halves, call merge sort on each half, assume that magically, they are going to come back sorted, and then you merge the sorted halves. How much time does merging take? OK. So the recursion for the running time of merge sort? Why does it take n time? Just too large? Is not it the finger thing where you take each element, and you are like, this one, is that greater or less than, then you put it in the array. So you get Please take my word for it that it is order n. I will explain it and then I will be confused. OK, so order n. What is the recursion? Do not give me the solution because then I ca not do the trees anymore, so give me the recursion before it is solved. Give me the recurrence formula. So it starts with T of N, right? It starts with N over two plus N, I think. Perfect. So you take the array, you split it into two, you call merge sort on the two halves of the arrays. So you call merge sort twice. That is why you have a two here. The two matters. Without it, you get a different answer. And when you call it, the arrays that you give it are half the size, and then merge takes order and time. Splitting depends on what you are using to store your arrays. Can be constant time or it can be order N. So the time wo not change because of split. How do we solve this recurrence? The recursion tree method says that we are going to draw a call graph. So we start out with a call to merge sort with an array of size N. Then it is going to call merge sort again, but after the array is split. So it is going to call merge sort twice, size is N over two point This guy gets an array of N over 2, calls merge sort. Two arrays, sizes N over 4, N over four point This does the same. So this goes on forever and ever and ever until at some point we reach our base case. So we are going to have a bunch of calls here where the array size is? What is our base case? one point Excellent. So this is the call graph for merge sort, and let is put the base case here so we know what we are talking about. T of one is theta one point Now inside the nodes, we are going to put the cost for each call without counting the sub call, so the children here. That is this guy here, except instead of order N, I will write CN. Remember how sometimes we use CN instead of the order of notation? The reason we do that is if I put in the asymptotic notation, then we are going to be tempted to sum them up. You are allowed to sum terms using asymptotic notation as long as there is a finite number of them, but here, it turns out there is an infinite number of them. Also, if you go this way, you can never go wrong. You always get the right answer, so that is why we switch from order N to CN. In order to merge sort an array of size N, we are going to merge sort two arrays of size N over two and then spend CN time on doing the merge. What are the costs here? To sort an array of N over 2, what is the cost outside the cost to merge? C of N over two point Perfect. C times N over two point C times N over two point How about here? C times N over four point Perfect. CN over four point My nodes are really ugly. I should have drawn them like this from the beginning. CN over four point There you go. How about down here? C of N over two to the i. You are going on step ahead. We will do that right next. C of N over log N, right? Because they are log N levels, so Let is not worry about the number of levels. You are ruining my steps. I was going to get to that two steps after this. Is it just C? Yep. So array size is 1, right? So the cost is C. C, C, C, C. OK, you guys got it if you are thinking of levels already. The next thing I want to do is I want to figure out how many levels I have in this tree. Why do I care about that? The answer for T of N is the sum of all these costs in here because the cost of merge sorting an array of size N is the merge sort plus the costs for sorting the two arrays. And the nodes here keep track of all the time spent in recursive sub calls, so if we can add up everything up, we have the answer to T of N. It turns out the easiest way to do that is to sum up the cost at each level because the costs are this guy copied over here. For a level, they tend to be the same, so it is reasonably easy to add them up, except in order to be able to add those up, you have to know how many levels you have. So how do I know how many levels I have? Someone already told me log N. How do I get to that log N? So when I get to the bottommost level, the number has to be 1, the number next to the node, because that is my base case. When I have a one element array, it is sorted, I am done. I return. So I can say that for each level, the number next to the node is something as a function of L. Here, I am going to say that this is N over 1, which is N over two to the zero power. And this is N over 2, so it is N over two to the first power. This is N over two to the second, and so on and so forth. It might not be obvious if you only have two levels. I do not want to draw a lot on the board because I do not have a lot of space and I had get my nodes all messed into each other. If it takes more than two levels to see the pattern, go for it. Expand for three levels, four levels, five levels, whatever it takes to get it right on a Pset or on a test. So you see the pattern, then you write the formula for the node size at the level. And assuming this pattern holds, we see that the size of a node at level l, the size is two N over two to the l minus one point Fair enough? You can say N over two to the l, and forget that there is a minus 1, and then the asymptotics will save you, so it is no big deal, but this is the real number. So that means that at the bottommost level, at level l, this size is going to be one point N over two to the l minus one equals one point So now this is an equation, so I can solve for l. I pull this on the right side, N equals two to the l minus 1, so l minus one equals anyone? The inverse of an exponential? I was not paying attention. Sorry. Log N. The inverse of an exponential is a logarithm. Keep that in mind for solving six point zero zero six problems. l minus one is log N so l is log n plus 1, roughly log n. I could use log n plus one and go through the math. It is a bit more painful and, because we are using asymptotics, it does not really matter. So now we know how many levels we have. Let is see what is the cost at the level. So all the calls at a certain level, what is the sum of the costs? For this level, what is the cost? CN. And That was the easy question. Just the root, right? How about this level? Because I have two nodes, the cost in each node is CN over two point How about this level? Four levels, each level CN over four point How about the bottom level? CN. Why is it CN? Because there are N arrays of size one point N arrays of size one point Excellent. A cute argument I heard once is you start out with N, you split it into N over two and N over two point Then you split this guy in N over 4, N over 4, so this is like conservation of mass. If you start with N and here, you do not end up with N total, then you lost some element somewhere on the way. So CN. CN, CN, CN, CN. I think I see a pattern. I think it is reasonable to say that for every level, it is CN. And if you write the proof, you can prove that by using math instead of waving hands. So CN times the number of levels, right? The answer for this guy is C of N is CN times l. What is l? N log N. Roughly. OK So order of N log N. C becomes order of, l is order of log N, N stays the same. Any questions? Are people getting it or did I confuse you even more? We got it. OK, sweet. Thank you for the encouragement. So this gets you through problem one of Pset two point So in this case, the tree is nicely balanced. The cost at each level is the same. When talked about recursion trees in lectures, he showed two more trees, one where pretty much all the cost was up here the cost of the children was negligible and one tree where all the cost was concentrated here, so the cost of all the inner nodes was negligible and the leaves were doing all the real work. So do not be scared if your costs are not the same. Just sum them up and you will get to the right answer. Now I am going to talk about binary search trees, except I will make a five minute general talk about data structures before I do that. So we use the term data structures. I think we covered it well, and I want to give you a couple of tips for dealing with them on Pset one point A data structure is a bunch of algorithms that help you store and then retrieve information. You have two types of algorithms. You have queries, and you have updates. You start out with an empty data structure, like an empty binary search tree or an empty list, and then you throw some data at it. That is when you update it. Then you ask it some questions, and that is when you query it. Then maybe you throw more data at it, so you do more updates, and you ask more questions, so you do more queries. What are the queries and the updates for the binary search trees that we talked about in lecture? A query would be like, what is your right child, what is your left child? So that is for a node. What are you looking for? I am looking for something for the entire infrastructure. So for the entire tree, what is a question that you would ask the tree? Max. OK. Min. Next larger. Next larger. Are you looking at the nodes? Is there an are you balanced question? Well, I would say that the most popular operation in a binary search tree is Search, which looks for we call it Find in the code because most code implementations call it Find nowadays, but binary search tree. What are you going to do in it? You search for a value. That is why it has the Search in binary search. So queries are operations where you ask questions to the data structure and it does not change. How about updates? What did we learn for updates? Insert. Excellent. So Insert was covered in lecture, and we are doing Delete today. So data structures have this property that is called the representation invariant, RI, or Rep Invariant. Actually, before I get there, the rep invariant says that the data in the data structures is organized in this way, and as long as it is organized in this way, the data structure functions correctly. Can someone guess for a sorted array what is the representation invariant? It can mean sorted. Yeah. A sorted array should be sorted. Sounds like a very good rep invariant. So the elements should be stored an array. Every element should be smaller than any element after it. And as long as the rep invariant holds, so as long as elements are stored in the right way in the data structure, the queries will return the right results. If the rep invariant does not hold, then God knows what is going to happen. What can you do in a storage array as long as the rep invariant holds? Sorted array. What is the reason why I would have a sorted array? What can I do that is fast in a sorted array? Min and Max. I can do that very fast. That is good. What is the running time? A constant. Perfect. Min you look at the beginning, Max you look at the end. Yes? Binary search. Binary search. That is the other reason for that. So binary search runs in order log N time, does not have to look at most of the array, tells you whether an element is there are not. Now, what if the array is unsorted? Will the algorithm work? It might say something is not there when it actually is there. You can do binary search on a non sorted array. So if the rep invariant does not hold, your queries might give you a wrong answer. How about updates? How do you search something in a sorted list? You find where it should go and you move everything. Yep. So you have to move everything, make room for it, and put it there so that the array is still sorted at the end. You ca not just append things at the end, even though that would be faster and lazier and less code. When you do an update to a data structure, you have to make sure that the rep invariant still holds at the end. Sort of a correctness proof for an update algorithm says that if the rep invariant holds at the beginning, the rep invariant is guaranteed to hold at the end. Why do we care about this rep invariant stuff? Suppose you have a problem, say on the next Pset, that asks you to find the place that is slow in your code and then speed it up. And suppose you recognize the data structure there, and you say that is inefficient, and you want to implement another data structure that would be more efficient. You are going to implement it. You might have bugs in an update. How do you find the bugs? Queries give you the wrong answers. You might do queries a long time after you do updates, and you are not going to know which update failed. The right way to do this is you implement the method called Check RI that is what I call it so check the representation invariant. And that method walks through the entire data structure and make sure that the rep invariant holds, and if it does not, it raises an exception because you know that whatever you try to do from there is not going to work, so there is no reason to keep going. So at the end of every update, you add a call to this Check RI method until you are sure that your code is correct. And after you are done debugging your code, you remove this method and you submit the code. Why do I want to remove the method? It might be painfully slow and inefficient, much slower than the actual queries and updates. For example, let is take a heap. Do people remember heaps from lecture? What is the query for a heap? Say you have a max heap. What is a query? Where is the max? OK, cool. So for a max heap, a query would be max. Running time? Constant. Perfect. Constant. What do you do? Look at the top? Yeah, exactly. OK. Sweet. So what are the two popular updates in a max heap? There would be Insert as well. OK. Insert. And did we teach you general delete? Usually Extract Max is simpler. That is all you need. What is the running time for Insert? Do people remember heaps? I think it was per N, but I am not completely sure. Anyone else? It is not. Life would be bad if it would be N. N squared? No. It is better than N, so you guys are doing a binary search over the few running times that I gave you earlier. add to the N, and then you compare your neighbor, and then you . If it is an array, there is not So conceptually, a heap looks like this. And yeah, it becomes an array eventually, but let is look at it this way. It is a full binary tree. Binary tree means that each node has at most two children, and full means that every level except for the last level is completely populated. So every internal node has exactly two children, and in here, every node except for some nodes and then some nodes after it will not have. Everything to the left is fully populated, and then at some point, you stop having children. It turns out that this is easy to store in an array, but I will not go over that. Instead, I want to go over inserting. What is the rep invariant for a heap? The max in the top, right? Well, for max heap, and then the two children are less than the next node. All right. So the guy here has to be bigger than these guys, then the guy here has to be bigger than these guys, and so on and so forth. And if you use induction, you can prove that if this is bigger than this, it has to be bigger than these guys, and bigger than these guys, and bigger than everything, and it is a max. That is the reason why we have that rep invariant. So the way we insert a node is we add it at the bottom, the only place where we could add it. And then if this guy is bigger than this guy, the rep invariant is violated, so we swap them in order to fix that. Now the guy is here. If this is bigger than this, we do another swap. If this is bigger than this, we do another swap. So you are going to go from the bottom of the heap potentially all the way up to the root. So the running time of insert is order of the height of the heap. Now, the heap is a full binary tree. I said full. I keep saying full. The reason I care about full is that the full binary tree is guaranteed to have a height of log N. It is always log N, where N is the number of nodes. So inserting in a heap takes log N. I have a question. Did not they say that because it is in an array, then to find it oh no, I guess because you can still do the swaps. You can still do the swaps when you have it serialized in an array. You know that given an item is index, the parent is that index divided by two point So you add an element at the end of the array, and then you know what the parent is, and then you keep swapping and swapping and swapping towards the. You do not ever have to put it in and shift everything over. You are only swapping. Yep. You only swap. That is important. Thanks for asking. That is important. So log N. Extract max, take my word for it, also log N. What is the running time for checking the invariant in a heap? So to make sure that this guy is a heap, if I had numbers here, what would you have to do? You had have to look at every node. Yep. So running time? Theta of N. Yep. So if I am going to submit code for a heap where the operations are our order of log N, or order 1, but then each of these calls Check RI, that is going to be painfully slow because I am making the updates be order N instead of log N. So you are putting Check RI calls in every update. You debug your code. When you make sure it is correct, you remove those, and then you submit the Pset. Make sense? Sweet. And we looked a little bit at heaps, which is good. Binary search trees. So a binary tree is a tree where every node has at most two children. When we code this up, we represent a node as a Python object, and for a node, we keep track of the left child, of the right child, parent, and then this is a hollow tree. It is not very useful. This becomes useful when you start putting keys in the nodes so that you can find them and do other things with them. So each node has a key. Let me draw a binary search tree. Can people see this? So this is a binary tree. Can someone say something a bit more specific about it? It is unbalanced. OK. It is imbalanced. So that means that finding things all the way at the bottom is going to be expensive. What else? So I said it is a binary tree. Give me something more specific. So binary tree just means that every node has two children. There is a bit more structure in this guy. So if I look at the root, if I look at 23, all the nodes to the left are smaller. All the nodes to the right are bigger. Now, if I look at 8, all the nodes to the left are smaller, all the nodes to the right are greater. This additional rep invariant defines a binary search tree. This is what we talked about in class. BST. Why would I want to have this rep invariant? It sounds like a pain to maintain nodes with all these ordering constraints. What is the advantage of doing that? Search is fast. Yep. Search is fast. How do I do search? If you are looking for forty two or for 16, you had be like, oh, it is less than twenty three point I will get on this path. So start at the root, compare my key to the root. If it is smaller, go left. If it is bigger, go right. Then keep doing that until I arrive somewhere or until I arrive at a dead end if I am looking for fourteen point This is a lot like binary search. Binary search in an array, you look at the middle. If your key is smaller, go left. If your key is bigger, then go right. Let is look at the code for a little bit. Look at the BST Node Class, and you will see that it has the fields that we have up here. And look at the Find method, and this is pretty much the binary search code. Lines eight and nine have the return condition when you are happy and you found the key, and then line ten compares the key that you are looking for with the key in the node that you are at, and then lines 11, 14, 16, and nineteen are pretty much copy pasted, except one of them deals with the left case, the other one deals with the right case. What is the running time for Find? Would not it be log N, right? I wish. If this is all you have to do to get log N, then I would have to write a lot less code. So not quite log N. We will have to go through next lecture to get to log N. Until then, what is the running time? Order h. Yep. So you told me at the beginning it is unbalanced. Yeah. So then it is not going to be fast. OK, so order h. The reason why we care about h, and the reason we do not say order N, is because next lecture after we learn how to balance a tree, there is some magic that you can do to these binary search trees to guarantee that the height is order of log N. And then we will go through all the running times that we have and replace h with log N. Now, it happens that in this case, if you would have told me order N, I could not argue with you because worst case, searches are order N. Can someone give me a binary search tree that exposes this degenerate case? Yes? If it is completely unbalanced and every node is greater than the parent nodes. So give me some inserts that create it. Insert five point five point Insert ten point ten point Insert fifteen point fifteen point Insert twenty point Yep. And I could keep going. I could say, 1, 2, 3, 4, five point I could say 5, 10, fifteen point As long as these keep growing, this is basically going to be a list, so searching is order N. This is a degenerate case. Turns out it does not happen too often in practice. If you have random data, the height will be roughly log N. But in order to avoid those degenerate cases, we will be doing balanced trees later on. So we covered Find. We know it is order h. How do you insert, really quickly? Do you mean in searching when it is balanced or unbalanced? This guy. So the trees look exactly the same. If it is balanced, it is going to look more like that than like this. Actually, this is balanced. This is perfectly unbalanced. This is somewhere in the middle. If it is balanced, it is just going to look more like this, but it is still a binary search tree. How would you insert a node? Yes? Ca not you start at the root and find your way down, and then the first open child that you see that is applicable to your element, state it then? Yep. So if I wanted to insert 14, which way do I go? So you had look at 23, and you had say, it is less than 23, go left. You had look at eight point You had say, it is greater than eight point You had go right. Look at sixteen point You had say it is less, so you go left. 15, it is less. Then you have an open spot so you stick it there. Excellent. Thank you. Yes? I have a question. What if we want to insert 5? Then So if you want to insert who? five point Or actually no, we ca not. I am thinking, is there any case in which need to move a node? How would you insert 5? Let is see. What would you do for 5? For 5, then we had insert it to the right of 4, right? Smaller, smaller, greater, five point Right? So there would be no case in which we had need to swap nodes or something? No. You are thinking ahead. We will talk about that a little later when we get to deleting. As long as you follow a path in the tree, the path that finding would get you to, as soon as you hit a dead end, that is where your node belongs. Because you know next time you are going to search for it, the search is going to follow that path and find the node. Yes? If you have values are the same, like two nodes at the same number, does it matter which side you put it on? You do not. Oh, I see. It is more like you would only have four 1 is in the tree. Yes. So if you are trying to store keys and values, then what you had have to do if you want to allow multiple values for the same key is you have a linked list going off of this, which node becomes an array of values aside from the key. Smart question. Thank you. That trips you up every time you do actual code, so that is the right question to ask yourself when you are implementing this. Will I have duplicates? How do I handle them? We do not. We take the easy way out. So if you look at Insert, on the next page, you will see that the code is pretty much the Find code copy pasted, except when Self Left is None or Self Right is None, instead of returning, it creates a new node. Does that make sense to people? All right. So Delete is going to be the hardest operation for today. Before we do that, let is do a warm up operation. Let is say I want to implement Find Next Larger, also called Successor in some implementations. So I have a node. Say I have node 8, and I want to find the next key in the tree that is strictly larger than eight but smaller than anything else. So if I would take these nodes and write them down in order, I want to find the element that would go right after it. How do I do that? Do not cheat. Do not look at the code, or make my life easier and do searches. Go down one to the right, and you try to get down left as far as you can. OK. Very good. So I have a node, and it has some subtree here, so I can go to the right here, I can go all the way left. We have an operation that does this, and it is called Min for a tree. In order to find the minimum in a binary search tree, you keep going left. For example, in this case, you get 4, which is good. So the way you would code this up is if you have Min, you go to the right if you can, and then you call Min on the subtree. And you can see that lines three and four do exactly that. Good guess. But you can line one says case one, so you have the right answer for one case. Now we have to handle more difficult cases. What if instead, I go down a bunch of nodes, and I want to find the successor for this guy, for example, and there is nothing here. What do I do? So if I want to find the successor for 8, what do I do? Sorry. It has an answer. What if I want to find the successor for 4? Go up one. OK. Go up one. Why does that work? You know it is going to be greater. So I am going up right. So I know that everything here is guaranteed to be smaller, everything here is guaranteed to be greater than this guy. This guy is up right, so this is guaranteed to be greater than this, and everything here is guaranteed to be greater than this, and so on and so forth for the entire tree. So if I go up right, I am happy. I definitely found my answer. Now, what if I have something that looks like this, and I want to find the successor for this guy? There is none. In this case, there is none if there is nothing else here. What if I have this, but then I have this? So I came down this way. Are you saying you are calling on that last node? Yep. Find the larger? I guess you had just trace back up. And where do I stop? It affects the tree, so you go up one from there. You do not stop there. Why ca not I stop here? Because you know that that not necessarily. You know that everything in that long branch right there is less than that node. This is to the left of this guy, so this guy has to be greater than everything here, and then you can repeat the argument that we had before. So here, we could stop right away because we could branch left. In this case, you have to go up until you are able to go left and up. If you get to the root, then what happened? Then we are in this case, and you have no successor. So take a look at the code. The next larger, lines one through nine point Case two, six through 8, does exactly that. If I ca not go to my right and find the tree there, then I go up through my parent chain, and as long as I have to go up to the left, so as long as I am the right child of my parent, I have to keep going. The moment I find the parent where I am the left child, I stop. That is my successor. What if I would have to find the predecessor instead? So the element that is smaller than me but bigger than everything else in the tree. What would I do? It is just the opposite. Just the opposite. So how do I do the opposite? You can take the max of the left side tree, or traverse up, and if that is less than OK, so if I have a left subtree, fine. Call max on it and get the rightmost node there. If not, I go up, and when do I stop? When I go left or right? You had have to go right. Is that right? Yep. So last time, in this case, when I was going up, if I was going left, I had to keep going, and the moment I went right, I was happy and I stopped. What if I want to find the predecessor? It is the opposite, right? So I will go this way, and the moment I can go this way, I am done. How do you do this in code? Slightly tricky. Just slightly, I promise. . It is hard. What I would do is copy paste the code, replace left with right everywhere, and replace min with max. You get it done. So we talked about how the tree is symmetric, right? So every time, instead of saying left, you say right, and instead of saying min, you say max. That is how you do this. How do we do deletions? So suppose I am in this tree and I want to delete fifteen point What do I do? Kill it. Kill it. Very good. What if I want to delete 16? What do I do? You need to put fifteen where sixteen is. OK. So I would put fifteen here. So I had sixteen point Suppose I have a big tree here. Actually, let is go for an easier case. Let is say I have this tree here. So you are here, you have a big tree here, you do not have anything here, and you want to delete this guy. You know that everything less than the top node is going to be less than it, so you can just move that up. Everything less than this guy is also going to be less than this guy. So you are saying move the whole tree up. Yep. So the way we do that is we had take this node is left link and make it point here, and take this guy is parent link and make it point here, and this guy sort of goes away. So we have two cases for deleting. We have if you are a leaf, we will take you out. Sorry. I got confused. If you have one child and that child is in the same direction as your parent, then you can do this. What if you have one child, but it is a zigzag like this? What do you do? It is still greater than, so you do the same thing. Exactly. Same thing. Just change this guy, change this guy, and I am happy. So it does not matter if you have a zigzag or a straight line. It might help you think about it to convince yourself that the code is correct, but in the end, you do the same thing. Now, what if I want to delete node 8? So what if I have a nasty case where I want to delete this guy and it has children both on the left and on the right? You have to take 8, compare it to its parent and compare it to its right child, and see which one is greater in order to figure out which node gets replaced in its spot. OK. So there is replacing that is going to happen. The answer is really tricky. I always forget this when coding. Try to understand it, and if it does not work, refer to the textbook. When you forget it, because you will, refer to the textbook or to the internet. So what you do is I ca not just magically replace this node with one of the subtrees, but we talked right before this about Next Greater, so finding a node is successor. If this node has both a left subtree and a right subtree, then I know that if I call Find Successor on it, I am going to go somewhere inside here, and I am going to find a node somewhere in here all the way to the left that is this guy is successor. So what I am going to do is I am going to delete this node instead, and then I am going to take its key and put it up here. So if I want to delete 8, what I do is I find its successor, then I delete it, then I take the fifteen that was here you can see it, right? It is still there. Put it here. So the reason this works is that everything here is greater than this guy. Everything here is smaller than this guy. This is the next node that is greater than this guy, but everything else is bigger than it, right, because we wanted it to be a successor. So if I take this value and I put it up here, everything in here is still going to be greater than it. This is a successor of this guy, so everything here is still going to be smaller than the successor. Great. In order to do a delete, I find the successor, and then I call Delete on it. How do I know that this will end? How do I know that I am not going to go into a loop that runs forever? Because it is not It is acyclic, right? OK. First answer, good. Eventually, worst case, I am going to get to the maximum, and then not going on have to delete the successor anymore. Now, another thing to note here is that if this guy is the successor of this guy, it ca not have anything on the left, because if it would, then whatever is down here has to be bigger than this, and whatever is to the left of this node has to be smaller than this. But we said that this is the successor of this, so there is nothing here. So this will be one of the easy cases that we talked about. The successor either has no kids, or it has only one child, only one subtree. So then I can delete it using one of the easy cases. So in fact, worst case that happens in a delete is my node has two subtrees. Then I find the successor that is only going to have one subtree, I change my links there, and I am done. What is the running time for Delete? Is it order h, because you should do it all the way down to the bottom of the tree, right? You have the right answer. Let is see why it is order h. It has to be order h, right? Otherwise, the tree would be too slow. If it is order N, then it is bad. So why would Delete be order h? This was a heap, right, so I ca not use this. I am going to write delete here again. So the first thing you do is you have to search for the key, right? That is order h. Now, if it is a happy case, if it is case one or two, you change some links and you are done. What is the time for that? Constant. Constant. So happy case, order h for sure. Now sad case. If you have two children, what do you have to do after you realize that you have two subtrees? Find the successor. OK. What is the running time for finding a successor? Order h. Order h. Once I find the successor, what do I do? Call Delete on that, and what happens? It is a happy case or a sad case? It is a happy case. Happy case, a few links get swapped, constant time. So worst case, order h plus order h. Order h. So insertions are order h, deletions are order h. The first one. Because the second one is from finding the successor. What is the first one for? Finding the node for a key in the tree. So if I say Delete 8, then you have to find eight point If I give you the node, then you do not have that. Good question. It is a good question. Thank you. So that is insertion. That is deletion. Let is look at the code for Delete. Looks kind of long. So lines through 21, happy case or sad case? Try to do it by looking at the if instead of looking at the comments. So lines through twenty one for Delete. On this tree? Which tree, because there are two deletes? Oh really? Sorry. Why do we have two deletes? There is BST Delete and then there is BST Node Delete. So BST Delete. Finds the node, and then calls Delete on the node. And then if the node is a tree is root, then it updates the tree is root. So let is look at the nodes delete. Oh, I see. I think I was looking at the wrong one. Thank you. My Delete was much longer than yours. So lines three through 12, happy case or sad case? Look at the if on line three and tell me, what case is it going for? . If it does not have a left child or it does not have a right child, is that the happy case or the sad case? Happy. Happy case. So lines four through twelve handle the happy case. Lines fourteen through sixteen handle the sad case. Do lines fourteen through sixteen make sense? Find the successor, then swap the keys, then delete that successor. Now, lines four through eleven are pretty much what we talked about here, except I ca not draw arrows on the board and instead I have to change left and right links. Line four has to see if we are a left child or a right child, and then lines five through seven and nine through eleven are pretty much copy paste, swap left with right. And they changed the links like we changed them here. Do we have any questions on Deletes? So if the successor had a right child, then all you do, you just do the workaround thing where you just Yep. So the case that it does not have two children. As long as it does not have both children, you are in the happy case and you can do some link swapping. Are you guys burned out already? Fair enough. I left a part out. What I left out is how to augment a binary tree. So binary trees by default can answer the question, what is the minimum node in a tree in order h. You go all the way to the left, you find the minimum. That is the minimum. It turns out that if you make a node a little bit fatter, so if instead of storing, say, twenty three in this node, I store 23, and I store the fact that the minimum in my left subtree is 4, then it turns out that I can answer the question in constant time, what is the minimum? Oh gee, if you store the minimum here, of course you can retrieve it in constant time, right? The hard part is, how do you handle insertions and updates in the same time? So the idea is that if I have a node and I have a function here, say the minimum of everything, if I have two children, here they are fifteen and 42, and say the minimum in this tree is four and the minimum in this tree is. So if I already computed the function for these guys, how do I compute the function for this? and compare it? Yep. Take the minimum of these two guys, right? There are some special cases if you do not have a child. If you do not have a left child, then you are the minimum. But you write down those special cases, and you can compute this in how much time? Order h, right? What if I already computed the answer for the children? How much time does it take to compute the answer for a single node? Constant. Constant. OK. For a tree, though. For a tree, it is order h. Yeah. You are getting ahead. You are rushing me. You are not letting me finish. Are you saying that we store the minimum value? So for every Each node has a field that says what the minimum value is in that tree. Yep, exactly. So for each node, what is the minimum in the subtree. So if I add a node here, suppose I add three and I had my minimums, what changed? This subtree changed, this subtree changed, this subtree changed, and then this subtree changed. So I have to update the minimums here, here, here, here. Nothing else changed. Outside the path where I did the Insert, nothing changed, so I do not have to update anything. So what I do is after the Insert, I go back up and I re compute the values. So here, I will have three point I go back up 3, 3, three point You could when you are passing down, though. When you are going down that column, you can just compare it on the way down. You do not have to go back up, right? Yep. So the advantage of doing it the way I am saying it is that you can have other functions instead of minimum. As long as you can compute the function inside the parent in constant time using the function from the children, it makes sense to compute the function on the children first. There is an obvious function that I ca not tell you because that is on the Pset, but when you see the next Pset, you will see what I mean. So if you have a function where you know the result for the children and you can compute the result for the parent in constant time, then after you do the Insert, you go up on the path and you re compute the function. When you delete, what do you do? Same thing. Same thing. If this goes away, then this subtree changed, and then if there would be something else here, then this subtree changed, but nothing else changed. So whenever you do an Insert or a Delete, all you have to do is go back up the path to the parent and re compute the function that you are trying to compute. And that is tree augmentation. Does this make sense somewhat? That is it. So what you will find in lecture notes is a harder way of doing it that works for minimum, but what I told you works for everything. So do not tell people I told you how to do this for everything. Sure nobody is going to know. 
</body>
</html>