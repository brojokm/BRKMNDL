<html>
<body>
So today is lecture is on sorting. We will be talking about specific sorting algorithms today. I want to start by motivating why we are interested in sorting, which should be fairly easy. Then I want to discuss a particular sorting algorithm that is called insertion sort. That is probably the simplest sorting algorithm you can write, it is five lines of code. It is not the best sorting algorithm that is out there and so we will try and improve it. We will also talk about merge sort, which is a divide and conquer algorithm and that is going to motivate the last thing that I want to spend time on, which is recurrences and how you solve recurrences. Typically the recurrences that we will be looking at in double o six are going to come from divide and conquer problems like merge sort but you will see this over and over. So let is talk about why we are interested in sorting. There is some fairly obvious applications like if you want to maintain a phone book, you have got a bunch of names and numbers corresponding to a telephone directory and you want to keep them in sorted order so it is easy to search, mp3 organizers, spreadsheets, et cetera. So there is lots of obvious applications. There is also some interesting problems that become easy once items are sorted. One example of that is finding a median. So let is say that you have a bunch of items in an array a zero through n and a zero through n contains n numbers and they are not sorted. When you sort, you turn this into b zero through n, where if it is just numbers, then you may sort them in increasing order or decreasing order. Let is just call it increasing order for now. Or if they are records, and they are not numbers, then you have to provide a comparison function to determine which record is smaller than another record. And that is another input that you have to have in order to do the sorting. So it does not really matter what the items are as long as you have the comparison function. Think of it as less than or equal to. And if you have that and it is straightforward, obviously, to check that three is less than 4, et cetera. But it may be a little more complicated for more sophisticated sorting applications. But the bottom line is that if you have your algorithm that takes a comparison function as an input, you are going to be able to, after a certain amount of time, get B zero n. Now if you wanted to find the median of the set of numbers that were originally in the array A, what would you do once you have the sorted array B? Is not there a more efficient algorithm for median? Absolutely. But this is sort of a side effect of having a sorted list. If you happen to have a sorted list, there is many ways that you could imagine building up a sorted list. One way is you have something that is completely unsorted and you run insertion sort or merge sort. Another way would be to maintain a sorted list as you are getting items put into the list. So if you happened to have a sorted list and you need to have this sorted list for some reason, the point I am making here is that finding the median is easy. And it is easy because all you have to do is look at depending on whether n is odd or even look at B of n over two point That would give you the median because you had have a bunch of numbers that are less than that and the equal set of numbers that are greater than that, which is the definition of median. So this is not necessarily the best way, as you pointed out, of finding the median. But it is constant time if you have a sorted list. That is the point I wanted to make. There are other things that you could do. And this came up in Erik is lecture, which is the notion of binary search finding an element in an array a specific element. You have a list of items again a zero through n. And you are looking for a specific number or item. You could, obviously, scan the array, and that would take you linear time to find this item. If the array happened to be sorted, then you can find this in logarithmic time using what is called binary search. Let is say you are looking for a specific item. Let is call it k. Binary search, roughly speaking, would work like you go compare k to, again, B of n over 2, and decide, given that B is sorted, you get to look at 12 of the array. If B of n over two is not exactly k, then well, if it is exactly k you are done. Otherwise, you look at the left half. You do your divide and conquer paradigm. And you can do this in logarithmic time. So keep this in mind, because binary search is going to come up in today is lecture and again in other lectures. It is really a great paradigm of divide and conquer probably the simplest. And it, essentially, takes something that is linear a linear search and turns it into logarithmic search. So those are a couple of problems that become easy if you have a sorted list. And there is some not so obvious applications of sorting for example, data compression. If you wanted to compress a file, one of the things that you could do is to and it is a set of items you could sort the items. And that automatically finds duplicates. And you could say, if I have one hundred items that are all identical, I am going to compress the file by representing the item once and, then, having a number associated with the frequency of that item similar to what document distance does. Document distance can be viewed as a way of compressing your initial input. Obviously, you lose the works of Shakespeare or whatever it was. And it becomes a bunch of words and frequencies. But it is something that compresses the input and gives you a different representation. And so people use sorting as a subroutine in data compression. Computer graphics uses sorting. Most of the time, when you render scenes in computer graphics, you have many layers corresponding to the scenes. It turns out that, in computer graphics, most of the time you are actually rendering front to back because, when you have a big opaque object in front, you want to render that first, so you do not have to worry about everything that is occluded by this big opaque object. And that makes things more efficient. And so you keep things sorted front to back, most of the time, in computer graphics rendering. But some of the time, if you are worried about transparency, you have to render things back to front. So typically, you have sorted lists corresponding to the different objects in both orders both increasing order and decreasing order. And you are maintaining that. So sorting is a real important subroutine in pretty much any sophisticated application you look at. So it is worthwhile to look at the variety of sorting algorithms that are out there. And we are going to do some simple ones, today. But if you go and look at Wikipedia and do a Google search, there is all sorts of sorts like cocktail sort, and bitonic sort, and what have you. And there is reasons why each of these sorting algorithms exist. Because in specific cases, they end up winning on types of inputs or types of problems. So let is take a look at our first sorting algorithm. I am not going to write code but it will be in the notes. And it is in your document distance Python files. But I will just give you pseudocode here and walk through what insertion sort looks like because the purpose of describing this algorithm to you is to analyze its complexity. We need to do some counting here, with respect to this algorithm, to figure out how fast it is going to run in and what the worst case complexity is. So what is insertion sort? For i equals 1, 2, through n, given an input to be sorted, what we are going to do is we are going to insert A of i in the right position. And we are going to assume that we are sort of midway through the sorting process, where we have sorted A zero through i minus one point And we are going to expand this to this array to have i plus one elements. And A of i is going to get inserted into the correct position. And we are going to do this by pairwise swaps down to the correct position for the number that is initially in A of i. So let is go through an example of this. We are going to sort in increasing order. Just have six numbers. And initially, we have 5, 2, 4, 6, 1, three point And we are going to take a look at this. And you start with the index 1, or the second element, because the very first element it is a single element and it is already sorted by definition. But you start from here. And this is what we call our key. And that is essentially a pointer to where we are at, right now. And the key keeps moving to the right as we go through the different steps of the algorithm. And so what you do is you look at this and you have this is A of i. That is your key. And you have A of zero to 0, which is five point And since we want to sort in increasing order, this is not sorted. And so we do a swap. So what this would do in this step is to do a swap. And we would go obtain 2, 5, 4, 6, 1, three point So all that is happened here, in this step in the very first step where the key is in the second position is one swap happened. Now, your key is here, at item four point Again, you need to put four into the right spot. And so you do pairwise swaps. And in this case, you have to do one swap. And you get 2, 4, five point And you are done with this iteration. So what happens here is you have 2, 4, 5, 6, 1, three point And now, the key is over here, at six point Now, at this point, things are kind of easy, in the sense that you look at it and you say, well, I know this part is already started. six is greater than five point So you have to do nothing. So there is no swaps that happen in this step. So all that happens here is you are going to move the key to one step to the right. So you have 2, 4, 5, 6, 1, three point And your key is now at one point Here, you have to do more work. Now, you see one aspect of the complexity of this algorithm given that you are doing pairwise swaps the way this algorithm was defined, in pseudocode, out there, was I am going to use pairwise swaps to find the correct position. So what you are going to do is you are going to have to swap first one and six point And then you will swap one is over here. So you will swap this position and that position. And then you will swap essentially, do four swaps to get to the point where you have 1, 2, 4, 5, 6, three point So this is the result. 1, 2, 4, 5, 6, three point And the important thing to understand, here, is that you have done four swaps to get one to the correct position. Now, you could imagine a different data structure where you move this over there and you shift them all to the right. But in fact, that shifting of these four elements is going to be computed in our model as four operations, or four steps, anyway. So there is no getting away from the fact that you have to do four things here. And the way the code that we have for insertion sort does this is by using pairwise swaps. So we are almost done. Now, we have the key at three point And now, three needs to get put into the correct position. And so you have got to do a few swaps. This is the last step. And what happens here is three is going to get swapped with six point And then three needs to get swapped with five point And then three needs to get swapped with four point And then, since three is greater than 2, you are done. So you have 1, 2, 3, 4, 5, six point And that is it. So, analysis. How many steps do I have? n squared? No, how many steps do I have? I guess that was not a good question. If I think of a step as being a movement of the key, how many steps do I have? I have theta n steps. And in this case, you can think of it as n minus one steps, since you started with two point But let is just call it theta n steps, in terms of key positions. And you are right. It is n square because, at any given step, it is quite possible that I have to do theta n work. And one example is this one, right here, where I had to do four swaps. And in general, you can construct a scenario where, towards the end of the algorithm, you had have to do theta n work. But if you had a list that was reverse sorted. You would, essentially, have to do, on an average n by two swaps as you go through each of the steps. And that is theta n. So each step is theta n swaps. And when I say swaps, I could also say each step is theta n compares and swaps. And this is going to be important because I am going to ask you an interesting question in a minute. But let me summarize. What I have here is a theta n squared algorithm. The reason this is a theta n squared algorithm is because I have theta n steps and each step is theta n. When I am counting, what am I counting it terms of operations? The assumption here unspoken assumption has been that an operation is a compare and a swap and they are, essentially, equal in cost. And in most computers, that is true. You have a single instruction and, say, the x86 or the MIPS architecture that can do a compare, and the same thing for swapping registers. So perfectly reasonably assumption that compares and swaps for numbers have exactly the same cost. But if you had a record and you were comparing records, and the comparison function that you used for the records was in itself a method call or a subroutine, it is quite possible that all you are doing is swapping pointers or references to do the swap, but the comparison could be substantially more expensive. Most of the time and we will differentiate if it becomes necessary we are going to be counting comparisons in the sorting algorithms that we will be putting out. And we will be assuming that either comparison swaps are roughly the same or that compares are and we will say which one, of course that compares are substantially more expensive than swaps. So if you had either of those cases for insertion sort, you have a theta n squared algorithm. You have theta n squared compares and theta n squared swaps. Now, here is a question. Let is say that compares are more expensive than swaps. And so, I am concerned about the theta n squared comparison cost. I am not as concerned, because of the constant factors involved, with the theta n squared swap cost. This is a question question. What is a simple fix change to this algorithm that would give me a better complexity in the case where compares are more expensive, or I am only looking at the complexity of compares. So the theta whatever of compares. Anyone? Yeah, back there.  You could compare with the middle. What did I call it? I called it something. What you just said, I called it something. Binary search. Binary search. That is right. Two cushions for this one. So you pick them up after lecture. So you are exactly right. You got it right. I called it binary search, up here. And so you can take insertion sort and you can sort of trivially turn it into a theta n log n algorithm if we are talking about n being the number of compares. And all you have to do to do that is to say, you know what, I am going to replace this with binary search. And you can do that and that was the key observation because A of zero through i minus one is already sorted. And so you can do binary search on that part of the array. So let me just write that down. Do a binary search on A of zero through i minus 1, which is already sorted. And essentially, you can think of it as theta log i time, and for each of those steps. And so then you get your theta n log n theta n log n in terms of compares. Does this help the swaps for an array data structure? No, because binary search will require insertion into A of zero though i minus one point So here is the problem. Why do not we have a full fledged theta n log n algorithm, regardless of the cost of compares or swaps? We do not quite have that. We do not quite have that because we need to insert our A of i into the right position into A of zero through i minus one point You do that if you have an array structure, it might get into the middle. And you have to shift things over to the right. And when you shift things over to the right, in the worst case, you may be shifting a lot of things over to the right. And that gets back to worst case complexity of theta n. So a binary search in insertion sort gives you theta n log n for compares. But it is still theta n squared for swaps. So as you can see, there is many varieties of sorting algorithms. We just looked at a couple of them. And they were both insertion sort. The second one that I just put up is, I guess, technically called binary insertion sort because it does binary search. And the vanilla insertion sort is the one that you have the code for in the doc dis program, or at least one of the doc dis files. So let is move on and talk about a different algorithm. So what we had like to do, now this class is about constant improvement. We are never happy. We always want to do a little bit better. And eventually, once we run out of room from an asymptotic standpoint, you take these other classes where you try and improve constant factors and get 10, and 5, and 1, and so on, and so forth. But we will stick to improving asymptotic complexity. And we are not quite happy with binary insertion sort because, in the case of numbers, our binary insertion sort has theta n squared complexity, if you look at swaps. So we had like to go find an algorithm that is theta n log n. And I guess, eventually, we will have to stop. But Erik will take care of that. There is a reason to stop. It is when you can prove that you ca not do any better. And so we will get to that, eventually. So merge sort is also something that you have probably seen. But there probably will be a couple of subtleties that come out as I describe this algorithm that, hopefully, will be interesting to those of you who already know merge sort. And for those of you who do not, it is a very pretty algorithm. It is a standard recursion algorithm recursive algorithm similar to a binary search. What we do, here, is we have an array, A. We split it into two parts, L and R. And essentially, we kind of do no work, really. In terms of the L and R in the sense that we just call, we keep splitting, splitting, splitting. And all the work is done down at the bottom in this routine called merge, where we are merging a pair of elements at the leaves. And then, we merge two pairs and get four elements. And then we merge four tuples of elements, et cetera, and go all the way up. So while I am just saying L terms into L prime, out here, there is no real explicit code that you can see that turns L into L prime. It happens really later. There is no real sorting code, here. It happens in the merge routine. And you will see that quite clearly when we run through an example. So you have L and R turn into L prime and R prime. And what we end up getting is a sorted array, A. And we have what is called a merge routine that takes L prime and R prime and merges them into the sorted array. So at the top level, what you see is split into two, and do a merge, and get to the sorted array. The input is of size n. You have two arrays of size n over two point These are two sorted arrays of size n over two point And then, finally, you have a sorted array of size n. So if you want to follow the recursive of execution of this in a small example, then you will be able to see how this works. And we will do a fairly straightforward example with eight elements. So at the top level before we get there, merge is going to assume that you have two sorted arrays, and merge them together. That is the invariant in merge sort, or for the merge routine. It assumes the inputs are sorted L and R. Actually I should say, L prime and R prime. So let is say you have 20, 13, 7, and two point You have 12, 11, 9, and one point And this could be L prime. And this could be R prime. What you have is what we call a two finger algorithm. And so you have got two fingers and each of them point to something. And in this case, one of them is pointing to L. My left finger is pointing to L prime, or some element L prime. My right finger is pointing to some element in R prime. And I am going to compare the two elements that my fingers are pointing to. And I am going to choose, in this case, the smaller of those elements. And I am going to put them into the sorted array. So start out here. Look at that and that. And I compared two and one point And which is smaller? one is smaller. So I am going to write one down. This is a two finger algo for merge. And I put one down. When I put one down, I had to cross out one point So effectively, what happens is let me just circle that instead of crossing it out. And my finger moves up to nine point So now I am pointing at two and nine point And I repeat this step. So now, in this case, two is smaller. So I am going to go ahead and write two down. And I can cross out two and move my finger up to seven point And so that is it. I wo not bore you with the rest of the steps. It is essentially walking up. You have a couple of pointers and you are walking up these two arrays. And you are writing down 1, 2, 7, 9, 11, 12, 13, twenty point And that is your merge routine. And all of the work, really, is done in the merge routine because, other than that, the body is simply a recursive call. You have to, obviously, split the array. But that is fairly straightforward. If you have an array, A zero through n and depending on whether n is odd or even you could imagine that you set L to be A zero n by two minus 1, and R similarly. And so you just split it halfway in the middle. I will talk about that a little bit more. There is a subtlety associated with that that we will get to in a few minutes. But to finish up in terms of the computation of merge sort. This is it. The merge routine is doing most, if not all, of the work. And this two finger algorithm is going to be able to take two sorted arrays and put them into a single sorted array by interspersing, or interleaving, these elements. And what is the complexity of merge if I have two arrays of size n over 2, here? What do I have? n. n. We will give you a cushion, too. theta n complexity. So far so good. I know you know the answer as to what the complexity of merge sort is. But I am guessing that most of you wo not be able to prove it to me because I am kind of a hard guy to prove something to. And I could always say, no, I do not believe you or I do not understand. The complexity and you have said this before, in class, and I think Erik is mentioned it the overall complexity of this algorithm is theta n log n And where does that come from? How do you prove that? And so what we will do, now, is take a look at merge sort. And we will look at the recursion tree. And we will try and there are many ways of proving that merge sort is theta n log n. The way we are going to do this is what is called proof by picture. And it is not an established proof technique, but it is something that is very helpful to get an intuition behind the proof and why the result is true. And you can always take that and you can formalize it and make this something that everyone believes. And we will also look at substitution, possibly in section tomorrow, for recurrence solving. So where we are right now is that we have a divide and conquer algorithm that has a merge step that is theta n. And so, if I just look at this structure that I have here, I can write a recurrence for merge sort that looks like this. So when I say complexity, I can say T of n, which is the work done for n items, is going to be some constant time in order to divide the array. So this could be the part corresponding to dividing the array. And there is going to be two problems of size n over two point And so I have two T of n over two point And this is the recursive part. And I am going to have c times n, which is the merge part. And that is some constant times n, which is what we have, here, with respect to the theta n complexity. So you have a recurrence like this and I know some of you have seen recurrences in 6. 042. And you know how to solve this. What I had like to do is show you this recursion tree expansion that, not only tells you how to solve this occurrence, but also gives you a means of solving recurrences where, instead of having c of n, you have something else out here. You have f of n, which is a different function from the linear function. And this recursion tree is, in my mind, the simplest way of arguing the theta n log n complexity of merge sort. So what I want to do is expand this recurrence out. And let is do that over here. So I have c of n on top. I am going to ignore this constant factor because c of n dominates. So I will just start with c of n. I want to break things up, as I do the recursion. So when I go c of n, at the top level that is the work I have to do at the merge, at the top level. And then when I go down to two smaller problems, each of them is size n over two point So I do c times n divided by two. So this is just a constant c. I did not want to write thetas up here. You could. And I will say a little bit more about that later. But think of this cn as representing the theta n complexity. And c is this constant. So c times n, here. c times n over 2, here. And then when I keep going, I have c times n over 4, c times n over 4, et cetera, and so on, and so forth. And when I come down all the way here, n is eventually going to become one or essentially a constant and I am going to have a bunch of c is here. So here is another question, that I had like you to answer. Someone tell me what the number of levels in this tree are, precisely, and the number of leaves in this tree are, precisely. The number of levels is log n plus one point Log n plus one point Log to the base two plus one point And the number of leaves? You raised your hand back there, first. Number of leaves. I think n. Yeah, you are right. You think right. So one plus log n and n leaves. When n becomes 1, how many of them do you have? You are down to a single element, which is, by definition, sorted. And you have n leaves. So now let is add up the work. I really like this picture because it is just so intuitive in terms of getting us the result that we are looking for. So you add up the work in each of the levels of this tree. So the top level is cn. The second level is cn because I added 12 and 12, cn, cn. Wow. What symmetry. So you are doing the same amount of work modulo the constant factors, here, with what is going on with the c1, which we have ignored, but roughly the same amount of work in each of the levels. And now, you know how many levels there are. It is one plus log n. So if you want to write an equation for T of n, it is one plus log n times c of n, which is theta of n log n. So I have mixed in constants c and thetas. For the purposes of this description, they are interchangeable. You will see recurrences that look like this, in class. And things like that. Do not get confused. It is just a constant multiplicative factor in front of the function that you have. And it is just a little easier, I think, to write down these constant factors and realize that the amount of work done is the same in each of the leaves. And once you know the dimensions of this tree, in terms of levels and in terms of the number of leaves, you get your result. So we have looked at two algorithm, so far. And insertion sort, if you talk about numbers, is theta n squared for swaps. Merge sort is theta n log n. Here is another interesting question. What is one advantage of insertion sort over merge sort?  What does that mean? You do not have to move elements outside of. That is exactly right. That is exactly right. So the two guys who answered the questions before with the levels, and you. Come to me after class. So that is a great answer. It is in place sorting is something that has to do with auxiliary space. And so what you see, here and it was a bit hidden, here. But the fact of the matter is that you had L prime and R prime. And L prime and R prime are different from L and R, which were the initial halves of the inputs to the sorting algorithm. And what I said here is, we are going to dump this into A. That is what this picture shows. This says sorted array, A. And so you had to make a copy of the array the two halves L and R in order to do the recursion, and then to take the results and put them into the sorted array, A. So you needed in merge sort you needed theta n auxiliary space. So merge sort, you need theta n extra space. And the definition of in place sorting implies that you have theta one constant auxiliary space. The auxiliary space for insertion sort is simply that temporary variable that you need when you swap two elements. So when you want to swap a couple of registers, you gotta store one of the values in a temporary location, override the other, et cetera. And that is the theta one auxiliary space for insertion sort. So there is an advantage of the version of insertion sort we have talked about, today, over merge sort. And if you have a billion elements, that is potentially something you do not want to store in memory. If you want to do something really fast and do everything in cache or main memory, and you want to sort billions are maybe even trillions of items, this becomes an important consideration. I will say that you can reduce the constant factor of the theta n. So in the vanilla scheme, you could imagine that you have to have a copy of the array. So if you had n elements, you essentially have n extra items of storage. You can make that n over two with a simple coding trick by keeping 12 of A. You can throw away one of the L is or one of the R is. And you can get it down to n over two point And that turns out it is a reasonable thing to do if you have a billion elements and you want to reduce your storage by a constant factor. So that is one coding trick. Now it turns out that you can actually go further. And there is a fairly sophisticated algorithm that is sort of beyond the scope of six point zero zero six that is an in place merge sort. And this in place merge sort is kind of impractical in the sense that it does not do very well in terms of the constant factors. So while it is in place and it is still theta n log n. The problem is that the running time of an in place merge sort is much worse than the regular merge sort that uses theta n auxiliary space. So people do not really use in place merge sort. It is a great paper. It is a great thing to read. Its analysis is a bit sophisticated for double zero six point So we wont go there. But it does exist. So you can take merge sort, and I just want to let you know that you can do things in place. In terms of numbers, some experiments we ran a few years ago so these may not be completely valid because I am going to actually give you numbers but merge sort in Python, if you write a little curve fit program to do this, is 2. 2n log n microseconds for a given n. So this is the merge sort routine. And if you look at insertion sort, in Python, that is something like zero point two n square microseconds. So you see the constant factors here. If you do insertion sort in C, which is a compiled language, then, it is much faster. It is about twenty times faster. It is zero point zero one n squared microseconds. So a little bit of practice on the side. We do ask you to write code. And this is important. The reason we are interested in algorithms is because people want to run them. And what you can see is that you can actually find an n so regardless of whether you are Python or C, this tells you that asymptotic complexity is pretty important because, once n gets beyond about 4,000, you are going to see that merge sort in Python beats insertion sort in C. So the constant factors get subsumed beyond certain values of n. So that is why asymptotic complexity is important. You do have a factor of 20, here, but that does not really help you in terms of keeping an n square algorithm competitive. It stays competitive for a little bit longer, but then falls behind. That is what I wanted to cover for sorting. So hopefully, you have a sense of what happens with these two sorting algorithms. We will look at a very different sorting algorithm next time, using heaps, which is a different data structure. The last thing I want to do in the couple minutes I have left is give you a little more intuition as to recurrence solving based on this diagram that I wrote up there. And so we are going to use exactly this structure. And we are going to look at a couple of different recurrences that I wo not really motivate in terms of having a specific algorithm, but I will just write out the recurrence. And we will look at the recursion tree for that. And I will try and tease out of you the complexity associated with these recurrences of the overall complexity. So let is take a look at T of n equals two T of n over two plus c n squared. Let me just call that c no need for the brackets. So constant c times n squared. So if you had a crummy merge routine, and it was taking n square, and you coded it up wrong. It is not a great motivation for this recurrence, but it is a way this recurrence could have come up. So what does this recursive tree look like? Well it looks kind of the same, obviously. You have c n square you have c n square divided by 4 c n square divided by 4 c n square divided by 16, four times. Looking a little bit different from the other one. The levels and the leaves are exactly the same. Eventually n is going to go down to one point So you will see c all the way here. And you are going to have n leaves. And you will have, as before, one plus log n levels. Everything is the same. And this is why I like this recursive tree formulation so much because, now, all I have to do is add up the work associated with each of the levels to get the solution to the recurrence. Now, take a look at what happens, here. c n square c n square divided by 2 c n square divided by four point And this is n times c. So what does that add up to?  Yeah, exactly. Exactly right. So if you look at what happens, here, this dominates. All of the other things are actually less than that. And you said bounded by two c n square because this part is bounded by c n square and I already have c n square up at the top. So this particular algorithm that corresponds to this crummy merge sort, or wherever this recurrence came from, is a theta n squared algorithm. And in this case, all of the work done is at the root at the top level of the recursion. Here, there was a roughly equal amount of work done in each of the different levels. Here, all of the work was done at the root. And so to close up shop, here, let me just give you real quick a recurrence where all of the work is done at the leaves, just for closure. So if I had, magically, a merge routine that actually happened in constant time, either through buggy analysis, or because of it was buggy, then what does the tree look like for that? And I can think of this as being theta one point Or I can think of this as being just a constant c. I will stick with that. So I have c, c, c. Woah, I tried to move that up. That does not work. So I have n leaves, as before. And so if I look at what I have, here, I have c at the top level. I have 2c, and so on and so forth. 4c. And then I go all the way down to nc. And so what happens here is this dominates. And so, in this recurrence, the whole thing runs in theta n. So the solution to that is theta n. And what you have here is all of the work being done at the leaves. We are not going to really cover this theorem that gives you a mechanical way of figuring this out because we think the recursive tree is a better way of looking at. But you can see that, depending on what that function is, in terms of the work being done in the merge routine, you had have different versions of recurrences. I will stick around, and people who answered questions, please pick up you cushions. See you next time. 
</body>
</html>