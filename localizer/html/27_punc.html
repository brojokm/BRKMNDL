<html>
<body>
So did everyone turn in PSET1? Yes? Good. OK, so there is a PSET1 critique due in a few days. My advice? You already did PSET1. You remember what you wrote out on the proof. Look at the solution. Write one paragraph today, and you are done with it. Then you go focus on PSET2. If you leave it off until Tuesday, you are going to have to read your proof again, remember what you are thinking. It is a lot more work. Just do it now, get it out of the way, and put PSET1 behind you. Is the critique only for the proof? Or is this for all of them? Nope. Just the proof. So you have to compare your proof with our proof? Is there an assignment for that? Or do we just know to do it? Uh. PSET1 be Stellar. Oh, no, sorry. It is not in Stellar. It is on our new grading site that just went out. So you have to go to our new grading site, and you have to type in your critique there. And it is one paragraph. You should aim for one paragraph. If you are doing more than that, then you are doing something wrong. And it is LATEX Plus math mode. So you can use math mode, and that is about it. OK, any more questions about the critique? It is a new thing. We care about it because it will make our grading life easier. And because it will force you to look at the solutions and see what you understood and what you did not. So we care about it. Do not ignore it. Yes? Like, how much is it weighted? How much does it count toward the grade? If you do not have a critique, we will most likely give you a zero for proof. . If your proof is bad and your critique of the proof is good, then you might get something. If your proof is bad and you have no critique actually if your proof is whatever it is and you have no critique, you get a zero point Yeah. CHUCKLE Any more questions about that? OK. Who needs help remembering what the document distance problem is? OK. Everyone who went to lecture or to remembers. That is good. Who went to lecture last time? Cool. That is good. So we did insertion sort and merge sort from a theoretical standpoint. Today we are going to look at the code for the insertion sort and, if we have time, look at the code for merge sort, and use the same strategy as we did last time to analyze them, look at the running time, make sure the running time matches the theory, and see how pseudocode turns into Python. So you all have your listings. Last time in document distance, we covered Main, and we covered most of the functions except for count frequency. Can anyone remind me what the call graph looked like? So the call graph is the tree that I had up on the left, and it started at Main. Thank you. So Main calls word frequencies for file, which in turn calls? Well, it is probably line list. OK. And count frequency. So we pretend we do not see the read file called. We assume that the data is already in memory or that the call takes time that is proportional to the running to the length of the file. And we only look at get word from line list and count frequency. OK. Who else does Main call? Vector angle? And the vector angle? Inner product. OK. Let is put up the constants for for the document distance problem that we used last time. So we said that the document has W words. And then when you take that list of words and you turn it into a distance vector, you will get assigned to a document vector. You will get L elements, which basically means L unique words. So L is the document vector length. And we assume we are using a natural language like English, so all the words are bounded in size. Like five to ten characters, for example. And to make our life easier, we say all the words have the same size W. So w is the word length. Using these numbers, can anyone remind me what we said the costs for these methods are? And we did not analyze count frequency, so it is OK to not take my word for it and not tell me what I said last time. But I would like numbers for word frequencies from file, get word from line list, vector angle, and inner product. Was that mu squared? That was last time. OK. Which which one? Does anyone does anyone else want to try? Let is not do guessing. I will pull them up if nobody remembers. I spent an entire hour on that, and you guys did too. It was painful. I know. But we had to, like, you know, add them up, and then Yeah. So we did a lot of work for those numbers. So get word from line list was order of W squared. Does anyone remember why? What made it take so much time? Cause you append it to the word list. For, like, you add it to the end to go through. OK. So you add it. So what? So, like, every time you need it, you, like, do word list equals word list plus words in line. OK. Excellent. So get words from line list, line five. There is a plus there. And that plus sign messes up the performance. So that is why it is w squared. Count frequency, we did not cover it, you had to take my word for it. So we will cover it now. And, inner product. So suppose you have two vectors of line length L1 and L2. How much time does it take to compute the inner product? L1, L2 time. OK. So how much time does vector angle take? L1 L2 time. L1 L2. So it makes three calls, right? You get the two things. First, it computes the inner product of the two lists. And then it has to compute the inner product of each vector of each document vector with itself. Because that is what is on the bottom of the fraction. So what is the running time for that? Plus L1 squared, plus L2 squared. Plus L1 squared, plus L2 squared. So someone was really helpful last time and asked me, can you make this simpler with some math? And I said, I do not know so I do not think I will. But I looked at my, I looked at my high school math afterwards, and it turns out that these so L1 squared plus L2 squared are guaranteed to be greater than L1 L2 as long as these numbers are positive. And we are working with document list, so they are always positive. So this will go away. So let is assume count frequency is w squared or smaller. So the total running time for word frequencies from file is w squared. What is the running time for everything? Would just be, like, w squared plus L1 squared plus L2 squared? Almost. Actually, it depends on which one is greater, right? Well, almost. So here W works, assuming you get one document with W words. But word frequencies from files is called twice, once for each document. First document has W1 words, second document has W2 words. So what is the running time? W1 squared plus W2 squared. W1 squared plus W2 squared. So I take this, and I add this. Right? Except when I add this, if I want to add L1 squared, I know L1 is the number of unique documents of unique words in the document. And W1 is the total number of words in the document. W1 guaranteed to be greater or equal than L1, so it is going to dominate. I do not need to add it. Same for L2. This is it. OK. You guys do not seem to remember the numbers for these. So that means I did not torture you enough last time. So let is do more. Let is look at count frequency. And let is compute the cost for that. So what we did last time was, we went through each line of code. We thought, how much time does it take to execute the line once? And how many times does the line run? And then we compute the product of that, add everything up, and that is the cost for the function. First off, before I put numbers here, what does the method to do? It takes a list of words OK. For each item in that list, checks to see if it is you know, list of words that it is counted, right? OK. So you are telling me what the code does. Yeah. Try to look at Main or try to look at word frequencies for files. So look at it top down, and tell me what the purpose of it is. What is the goal? Making a list of and each object is a list with a word and a number. OK. Excellent. So big picture, I have the first document. I read it in. I break it up into words. And I have a list of words. That is what word frequencies for file gives me sorry, that is what you get words from line list gives me. List of words. The fox is in the hat. And this gets passed to count frequency, and count frequency gives me, you said, an object, which is a list. Of lists, where each of them has the word and how many times it shows up. So I would have the shows up twice, fox shows up once, is shows up once, in shows up once, and I need a shorter example hat shows up once. So it takes this and turns it into that. So on line 2, I have a list L that is initialized. And then, at the end, it is returned. So I am going to guess that L is going to look like this. Line 3, for new word in word list, iterates over the input. So iterates over this. And then, line four checks to see, for each new word, it looks at the list that I have under construction. So exam for example, if I ran through all the words and then I am trying to put in hat right now, I would not have it here. What line four does is, it looks at all the entries. And it says, if I can find the words so if I can find the word hat somewhere here then increment the number. If I ca not, then make a new entry and say that the word shows up once, because it is the first I see it. So this is what the code does. Now let is see how fast it does that. So line two initialize the output to an empty list. What is the cost for that? . Very good. How many times? For new word in word lists. Cost? . I know the cost is one point OK, so we are here, it is a bit confusing, right? We are saying that, oh, there is does iteration over a list. And each step of the iteration is constant time, but the iteration happens L times. I am sorry, not L times. The length of the list times. How many how many elements are in word lists? I heard a very low W, so I will pretend I heard it. Or I hope I heard W. So word list, the words I got from the document, W. How about the if. So, it looks at the word that I have oh. This code is confusing because I forgot a line, right? Pretend that between line oh, no, sorry, I did not. New word is new word is assigned in line three point So new word in line three is compared to the first element of the entry that is assigned in line four point So hat is compared with the, fox, so on, so forth. And if the comparison is true, it runs line six and seven point And if not, it keeps looping. So line 5, the if how, many times does it run? Just the comparison. W. W. Oh, no, no, no. No. I am thinking of line four point Oh. Yeah. I am getting confused too. So let is start with line four point Sorry. Should not do line five point new word, then you are not like, you are not going to run through that again. Yeah. Let is worry about that right afterwards. Let is do line four first. Sorry, I jumped over line four point So, line four definitely runs W times because it is inside the for loop from line three to line nine point So everything here will definitely run W times. But how many times does it run overall? So, line four iterates over all the entries here. How many times does that happen? one plus W over times 102, because it is just worst case, L the length of L increases by one every time.  OK, so I like that you started with worst case. Normally I would say exactly that. Worst case or W. But we had a different constant for the number of words that you have in the end. So let is say something a little bit better than W. Let is say, let is put the lower bound on it. So yeah, worst case, all the words are different. But what if they are not all different? And what if in the end I know I have L words? CLASS. Worst case. Almost. So, I know I have a W from the outer loop. For each word in the outer loop, how many times does the inner loop execute? How many times do I have to go through something in the inner list? So I know here I have W words, suppose here I have L elements in the vector. For each one of these, how many times do I have to go through So how many elements do I have to go through here? Depends on where you are, though. For the first word, you only have to go through one. Yep. But For the second word, you have to through Yep. But I heard the worst case. And I like that, because it is easier to reason about the worst case. And most of the time it is sort of like the average case. So then, length of list L. The length of the list, and that is L. Worst case, the first words that I see will be L different words. And then all the words that I see will be the same as the words that I saw before. So worst case, the list will grow to L very fast, and then I will keep seeing L L L. And I will ignore what was there in the beginning, and I will say L times. So I know the second list is bounded by L in length, the first list is bounded by W in length. So worst case this runs L times W times. And what is the cost of iterating? What is the difference between L and W? L is the document vector length, and W is the number of words. But is not the number of elements in the document vector the number of words? How about this case? What is W? 6, yeah. So L is the number of unique words in a document. And I heard a really cool argument that I liked last time. Does anyone remember? About L? If we are really dealing with a natural language like English, how many words do I have in English? Well, I think, at the max, there is actually around 250,000, but a lot of them are not used anymore. OK. So 250,000, right? Max. So that is a constant. If I have a document that contains all the writings of all the authors that were ever done, and say that is a billion words, L is still going to be 250,000. Right? So L can be very different from W. That is why we are keeping track of them separately. One W L times W. What is the cost of iterating? So we know how many times line four runs, but what is the cost of one step of iterating in the list? one point Very good. Line five point How many times does it run? W times L? Yep. Same as line 4, right? The if is run all the time. And lines five and six only run sometimes, but sorry, line six and seven only run sometimes, but line five runs all the time. What is the cost? We can say it is constant. We can say it is constant. I like we can say it is constant, but why is it that we can say it is constant? Why do not I just say 1, if this is an empty list. This is a number. Depends on the word length. OK. It depends on the word lis length, very good. And we are assuming that words are all the same length. OK. And? So, one times L W. Little w. OK, very good. So we do assume that all the words are the same length. But unless I tell you that the length is really small, which I did, you ca not say one point So, when you said we can say it is constant, it is right. We can say, but we also have to say why, or at least think why, that is the case. So it is W we are going to use W here and when we copy it here, we are going to forget about it. Can you put a top bar on the W, just so I can tell that it is not the other W. OK But you have to be responsible for reminding me to put that. OK. OK. So string comparison, not constant. If I have two very long strings that only differ in the last character, I have to go through them character by character by character until I find the last character that is different. Because until I look at that, the strings might be equal. So comparing two long strings takes time that is proportional to the length of the strings. OK, so line five costs w, tricky part, runs L times W part L times W times. How about line six and 7? I did not ask line 6, I asked line six and seven together, because there is a trick there. I think it is constant for line 6, right? Why? Cause it is a number. And one place in the entry. We have already grabbed the entry. So the cost of running it once is one point Good. I can tell you that that is the same case for seven point How many times do they run? This is the hard part. Wait, does break line break out of one loop? Yep. Break breaks out of the loop between lines four and seven point I have a question. Is L supposed to be not in line with if? Yes. It is supposed to be where it is. I will talk yes. So what happens is, that else is in line with a for. And if the for loop runs to completion, then it does get executed. If there is a break somewhere inside the for, then it does not get executed. So the idea behind that is, usually use this for finding stuff. So you iterate over a list, and when you find something, break out of the loop. You did something, you break out of the loop. If you did not find it, you can put an else and then say what code happens. And you do not have to write code on your own to check if you broke out of the loop. So if break executes, then it is going to take us out of the loop. It is going to ignore the else. And it is going to run the loop on line three again. So it is going to do another iteration. So line six and 7, how many times? W minus L? W minus L. Like, if it is the difference in number of words and number of unique words. Smart. You gave the precise answer right the first time. W minus L. Very good. And what happens once what happens once this runs? Why do I know why do I know that the if wo not be Oh. Sorry, I am getting myself confused. So it is going to run W minus L times total, right? Total times overall without this W thing here, so I should put an arrow and say Now suppose I did not notice this. Is there another way I can get the decent bounds? So this is the right, perfect answer. You have this, you are done. You do not need to think further. Suppose you do not have this. What else can you do? But we do not have what? If I did not notice that, hey, there are L words There are L new words, W words total, so W minus L words repeat themselves. So this is how many times the if is going to be true. If I did not have that, then I could see that line seven breaks out of the loop. So if that if runs once, then we are done. We are out of the loop. It is not going to run again. So a bound that is not as precise as the one you gave me is one times W, because it runs, at most, once per loop. Does people see this? So this is an easy way to cop out of thinking. And I do not like to think more than necessary, because you have finite time on a test or in life, and you do not want to spend too much time on one thing. We covered the loop. Let is look at else and append. I already got a helpful question, so I explained when the else would run. The running time for else. Else is a flow control flow statement. It is like break, so Python will keep track of whether a loop completed or not in constant time. I will give you that. That is in the cost model. How many times does this else run, at most? OK, L. Good. Perfect. How about line 9? Loop stops here. How about line 9? How many times does it run? That is easy. . Yep. And what does it do? It is an append. What is the cost for append? Constant. OK Line ten runs how many times? What is the cost? 1? You guys did not listen to me last time. So I was saying you have to look at the notes and you have to practice this. Because you have to have this model in your mind. So that when you are writing code, this has to happen automatically. You should not have to think explicitly about it. Because if you do, you are not going to do it. For the else, should not it be W tim I mean, it would be called W times not L times? Because you want to look at the outer loop and not the inner loop? So it can you call all once at the end of the total the inner for. Right? So, so it could be happen W times, maximum, not L times. Cause the L is the for loop that the else coincides with. And the else would only happen once for every total iteration of that. OK so you are proposing W as the bound for the else, right? Here. Yeah. So I could say, hey, it runs, at most, once for outer loop. So it is, at most, W times. This is a nice, easy argument. We have a bound. L is a tighter bound. And when I got L, what happened here was the same kind of thinking that you did earlier to get this. So this bound is good, this bound is tighter. This bound is good, this bound is tighter. And the argument behind this one is that, hey, this else only happens for new words. If there is no new if the word that I looked at is old, then break is going to execute. And else is not going to execute. So that is why I can say L. The beauty of asymptotics is that I can use either of the bounds and I will still get the correct running time. So I am not going to fuss over it too much. I like the tighter ones, because it means you guys are thinking. And you are looking at the algorithm, and you are understanding it. But if you do not have them, you will still get the correct running times. So I think that is nice. OK, let is get the running time for everything. Can someone do it in one step? Then let is do it step by step. So let is compute partial products. What are they? one point W. LW. L, W, little w with the bar. LAUGHING With the bar. Awesome. W minus L. W. L, L, one point OK, so if I add them up, this is all asymptotic. So the biggest one will dominate. In general, I can just take a max instead of doing actual addition. So who dominates here? The fourth one down? Fourth OK. Yep. So line five is the biggest time consumer in this algorithm. And I know it is W times w bar. So now I am going to copy it here. What did I say I will do when I am copying it here? . Yep. So I am assuming English five to ten character words. W L. And W L is smaller than w squared, so the assumption that I had before is correct. I do not have to change anything here. That is good. So we noticed last time, and already forgot by now, that the biggest problem in this whole implementation was the plus in get words from line list. Suppose we forgot about it and we have this big pile of code, how do I go about making it faster? Method one, go through every method. Do this. Compute the running times, and see which one is the slowest. Does this scale to 1,000 lines of code? Not so much. We are going to be giving you roughly 1,000 lines of code for PSET2, and we are going to ask it to make it faster. Do want to understand everything? No. Instead what you want to do is run the code through a profiler. So we have a computer. The computer can tell you which line takes the most time to run. So you do not have to do it on pen and paper. Whenever we can automate, do so. So we will teach you how to run a profiler. It is in the notes. And if you look in the code outputs right after, you will see a profiler output. So what that tells you is for each function, how much time does it take that is the total time? And there is the cumulative time, which is how much does it take together with its children? In this case for word frequencies from file, order of W this is how much time it takes including the functions it is called. So this is the cumulative time. Cumulative time is useful if I am during runtime analysis. Is not so useful if I am looking at where is the slowness in my program? Because if you look at cumulative time, you might see the slowness in one of the functions that get that word frequencies from file called. So the cumulative time for this is really big, but the total time the time that is spent inside it is not that bad. Instead if you look at total time, you will see that the worst function is surprise, surprise get words from line list. So five lines to look at hey, there is a plus there. I remember from lecture that plus copies over lists, and it is kind of slow, so maybe I should use something else. Does d remember what else we should use? We talked about that last recitation. Extend. Document distance two. The only difference between it an document distance one is get words from line list, line five point That plus turned into an extend. One character turned to six, so about eight keystrokes, 30 runtime improvement. Very good return on investment. Everything else is going to be harder. So let is look at that line, because that line dominated the running time of get words from line list. And think what is, then, your running time for it now that I have extend? Does anyone remember what get words from line list does? It gets the words out of the document. OK. It gets the word out of the document. So it reads a document that looks like a regular text file, and it gets this out of it. The way it does that is it goes through each line, reads the line as a string, breaks up the string into a list of words, and then combines all those lists of words together. Get words from string, line 4, returns the number of words in a line. Sorry, the list of words in the line, and then extend combines the lists together. Let is add some constants that we had last time. I think we had that K is the number of words per line. And Z is the number of total lines. So this K is actually W over Z. No, I do not like that. That is work. So Z is K W over K. And we argued that we are not going to talk about K too much, because a document needs to fit on a screen or needs to fit on a piece of paper. So the line length has to be finite, right? Otherwise, if I have a document that has 10,000 character lines, I ca not even write it on this board. Even though it is really long. So K, the number of words in a line, is going to be finite. But we will need it for this analysis. So line four returns a list with the words on a line. How many elements in that list? This returns a list with how many elements? Could K words on line. So even if I ask easy questions, you guys have to answer. Because otherwise I will stall until you do. So four gives me a list with K elements, and five appends that small list to the big list of words in the entire document. Before I used plus and that did something bad. Now I am using extend. What is the cost of one extend called? Constant? Order of K? I want to know your Python implementation. If Python did it like a linked list, like doubly linked lists, could be order constant? I like your question. So if Python lists were actually linked lists so if the name was not confusing then yes, merging two lists would be constant. But then accessing one element in the middle of a list would not be constant anymore. Say I want to access element number two hundred in a list of 10,000 elements. I have to go through two hundred elements. We did not do linked lists when we ran it, so do not worry about it. So they decided that it is less confusing to have lists actually be arrays. So Python lists, array in CLRS. That can use their storage contiguously? Yep. So when you copy, why ca not you copy a block? Why do you have to access each other? Does that make sense? So, you can copy a block, but in order to copy a block, you still have to move everything. So if your block is 10,000 elements, you still have to move 10,000 bytes times element size. And the CPU works on four bytes at a time or eight bytes at a time. OK. But this is the right kind of thing to be thinking about when you are doing the cost model. And this is what you want to have in your head when you are writing Python. So, good. I like your question. I wanted to say that at some point, but I did not get the occasion yet. Lists in Python are not lists in CLRS. OK. So with that long explanation, an extend is a list is a sequence of appends, right? If you have two lists and you want to extend the first list to the second list, extend basically goes through each element in the second list and calls append on the first list. The list is length K, the second list is length K, so K appends are going to happen. And append is constant time. Total cost, K. Now many times does line five run? . Very good. So what is the total running cost of the algorithm if this is the line that dominates? I do not want to do every other line, so I will promise that this is the line that dominates. What is the total running time? . OK. Very good. K times L. And that is? Oh, K. So I should not have said K times L, sorry. L is not the number of lines. L is the number here, so Z is the number of lines. So it is K times Z. Which means I am using bad letters, so please bear with me. We will forget about them in a minute. So K times Z equals? W. W. So what do I write here? . OK. Good. Very good. What do I write here? Word frequencies from file. W L. OK. What do I write here? W one point We have to put L one squared and L two squared back in. OK, do we? Well, the No. W, you want to always be bigger than L. I hope. Yep. So if I put it in, L one squared is L one L 1, which is smaller than W one L one point But I have to think about it before doing that. I ca not ignore this completely. So this is document distance two point The asymptotic complexity improved, the running time improved. Next thing that happens to make this faster is I am not giving you the profiler output, but you have to take my word for it that the longest methods are count frequency and inner product. So what I am going to do first is I am going to make inner product faster. But in order to do that, I have to make word frequencies for file slower. And that is because I happen to know an algorithm for inner product that is a lot faster, if only you can promise me that in this list, the words are ordered. The words are sorted. So what happens in that list one is, the moment I see a word, if it is not in the list I add it at the end. So the words here show up pretty much in the order that they show up in the file. Well, if instead I could have something that looks like what is it? Fox. In. His. Hat is somewhere here. So if these words would be in this order, together with the word that I am missing, then I can combine two lists. I can do an inner product a lot faster. Let is see how that would work. And I am already getting confused my words, so let is do a trick. Let is say that instead of words, we will use numbers. So instead of saying the, I am going to say the is the 50th word in the dictionary. So I am going to use number fifty point Because I want to write numbers. The numbers are easier to deal with. So say the first document that I have the fox is in the hat as words number 3, 4, 6, 8, and nine point And they show up twice, once, once, once. 9, once. And say I am trying to compute the inner product of this with a document that has word number two showing up once, word number three showing up once, word number six showing up once, word number seven showing up once, word number eight showing up once. OK, the algorithm for inner product that we talked about last time was, go through each element in one of the vectors, find an element with the same word in the second vector, and if you can find it, then take the number of times the words show up and multiply them. So here I have a 3, two point I would find this element here that has 3, one point I have these everywhere. And I take the two and the one and I multiply them. So the 3s have to be the same, then I think the two and the 1, and I multiply them. And for all the elements where that is case, I add up the results of the multiplication. So step one, go through each element in a vector. That is not going to get faster if this other vector is sorted, right? But the step of looking up the second element can be sped up. The first and easiest way I can speed this up is, hey, this is sorted. If I am looking up three, why not do a binary search? What would be the cost if I do that? So here I have L1 element, here I have L2 elements. What is the cost of doing one binary search here? Log of L2? OK. And how many times do I do a binary search? . So if I go through each element here and I do a binary search, which is a nice and easy algorithm that I can explain in ten seconds, I am already faster than L1 L2. So it is worth sorting. Now, the algorithm that we use in class takes time proportional to L1 plus L2. So that is even trickier, and it is going to take a bit more time to explain. Does did anyone understand the algorithm for class and wants to help me explain it? Did not think so. OK. So idea is that both of these vectors are sorted. So if I have a three here and I found my three here, next time when I get to 4, I know for sure that four is not going to be anywhere here, because this vector is sorted. Say I could not find 4, then I go to six point If six is here, when I have to look for 8, I know for sure that eight is not going to be anywhere up here. So what I do is, I have a pointer here that remembers, where is the last element that I have seen? Does this make sense to people? So when I start here and I look at 3, I have a pointer here that says, I did not see anything here. I look at 2, it is not three point It is smaller. I look at three point I found it, good. I do my product. Then I go look for the next element here, four point I look at six point six is bigger than four point So I know for sure that nothing below it is going to be four point So I can stop right here and keep my pointer here. Then I go to six here. And I look here. Where did I stop? I stop here. This element matches this. I do my product. Keep it in. Now I go to eight here. I was at six point six is smaller than eight point seven is smaller than eight point eight is equal to eight point I found something. Sweet. I do a product. And then I stop. And I look at the next element. I know for sure that nothing here is going to be 9, so I can keep looking down. I hit the end of my list. OK. Whatever I have down here it is going 9, 10, 11, twelve is not going to be in this list, because it was sorted. So I can stop. How do you keep going What algorithm so you use to keep looking down on L2? Plus one point So what if I keep going down. What if the left side was 3, 4, 6, 9243? 9,000, what? Just a big number. Right. Then you have to increment by one each time So I am not incrementing the number I am looking at. Here, I looked at six point Then I am looking for eight point And after I found 8, I am looking for 19,000. So I am going to go down, either until I find 90,000 or until I stop. So are you going to go down one at a time, Yep. why not do a binary search? Because if I do a binary search, the analysis that says it is fast is not going to work. It turns out that this gives me the optimal running time. If I do a binary search, suppose I have a list that is like this. 1, 2, 3, 4, 5, all the way down to 10,000. Ugh, I ca not write. And I have another list that is like this 1, 2, 3, 4, all the way down to 10,000. 1, one point I do a binary search, takes log N. I look at two point I do a binary search, takes almost log N. I look at 3, do a binary search, takes almost log N. So on, so forth. So this is But if your left list had been Log N plus log N. 10,000. Yeah. You had have taken N time. Yes. Well, this algorithm takes N time, even if I have to list like this. It takes 10,000 plus 10,000 time. Whereas this algorithm will take time that is actually proportional to 10,000 log 10,000. You believe me. So, a way to look at this is to do bounds and say, for the elements one through 5,000, it is going to do a binary search for more than 5,000 elements. So, the time the running time is definitely bigger than N over two log N over two point Constant. This becomes log N minus one and log N. That is a good question. I wondered about that the first time I saw merge sort too. And I was thinking, hey, I am going to do a binary search here because it is faster, and I am going to make a faster algorithm than anyone has ever seen. Well, if you do the analysis, not so much. But you need to think about it, and you need to know why that is true or that is not true. So I like your question. Thank you. So now let is get down so the plain old merge sort that everyone sorry, merge that everyone knows. So if we go through these one by one, how many times am I going to be advancing this pointer? In total? L2? L2 times. So this pointer can only go down, right? So worst case is going to go down L2 times. And then I am done with the list, return. How many elements am I going to look through? So how many times does this pointer going to advance? L1? This one. But I thought, like Then you get extra ones in between. What if L2 is bigger than L1? What if Oh, right. OK, never mind. The reason I said L2 was because You are thinking after I am going through this list, I am out, right? Right, that is what I thought. So, your answer works if this list, say, has ten elements and this has 10,000. And I go through this one really quickly. But if this list has ten elements and this list has 10,000, and they both start with one through 10, I have to say a one because that is a better bound. So I have to say this. Could you say the opposite value of the difference between the two of them? Because if you are going to stop at one has ten and the other one has 10,000, and let is say that only the first ten are actually equal, then you are going to go through that list, find all 10, and you stop. That would be 9,000 I could say about that if I am looking at one case, but the magic trick is let is we are looking at the worst case. So worst case, if I have ten elements, they will be all the way down in the other list. Or they wo not be there at all. And I have to go down through all the list. OK. So worst case, L1 plus L2. Let me see if I have any time left. Nope. So, what I would like you to do is go through insertion sort. Insertion sort matches the textbook. Look at the definition in the textbook, look at the code, convince yourself it is the same. Look at the running time, convince yourself it is N squared. Then look at inner product and convince yourself that this is what it does. Go through this line by line, see where they match, put the cost on. Make sure that the cost is L1 plus L2. And last, go through merge sort and notice that merge in six is exactly the same as inner product. So this pointer magic that I did here is exactly what is happening inside merge. And understand merge sort. Look at the textbook, look at the notes, and see how they match. OK. Thanks, guys. 
</body>
</html>