<html>
<body>
Good morning, everyone. So lecture three of four in the shortest path module and today we will finally confront our nemesis, which are negative cycles and negative edges. And we will describe an algorithm that is due to two different people. They did not collaborate to produce this algorithm. Bellman and Ford. This computes shortest paths in a graph with negative edges. And not only that, even in the graph has negative cycles in it, the algorithm will be correct in the sense that it will report the existence of a negative cycle and, essentially, abort the computation of shortest paths that are undefined. And for the few vertices that do not have negative cycles in between them and the source, the algorithm will report correct shortest paths. So it is a polynomial time algorithm. It is fairly easy to describe. And what we will do is describe it, analyze its complexity and, for once, we will do a formal proof of its correctness to show that it reports the existence of negative cycles if they do exist. And if they do not exist, it correctly computes shortest path weights. So recall that when we look at the general case of the shortest path problem. We are going to have, let is say, a vertex u that, in this case, happens to be our source. And let is say for argument is sake that we have a negative weight cycle like so. So let me to draw this in bold. And this happens to be a negative rate cycle. Let is assume that all of these edges have positive weights. Then, if you have an algorithm that needs to work on this type of graph, what you want to be able to do is to detect that this negative cycle exists. And you are going to, essentially, say if this vertex is v1, for example, you want to be able to say delta u v1 is undefined and similarly for v2, v3, et cetera. For all of these things, the shortest path lengths are undefined because you can essentially run through this negative cycle any number of times and get whatever shortest path weight you want. For this node, let is call that v0, we have delta u v0 equals two point And there is a simple path of length one in this case that gets you from u to v0. You do not encounter a cycle or negative cycle in between. So that is cool. All right? And of course, if you have a vertex over here, z, that ca not be reached from u then we are going to have delta uz being infinity. And you can assume at the beginning of these algorithms that the source in this case, I call the source u but the shortest path to u would be zero point And all of the other ones are infinity. And some of them may stay infinity. Some of them may obtain finite shortest path weights. And some of them will be undefined if you have a graph with negative cycles in it. So that is sort of the specification, if you will, of the requirements on the Bellman Ford algorithm. We want it to be able to do all of the things I just described. OK? So let is take a second look at our generic shortest path algorithm that I put up, I think, about a week ago. And this is a good review of our notation. But there are a couple more things I want to say about this algorithm that I did not get to last time. So you are given a graph and you set all of the vertices in the graph to have infinite shortest path weights, initially. Set the predecessors to be null. And then we will set d of s to be zero point That is your source. And the main loop would be something like repeat, select, and edge. And we have a particular way of selecting this edge. And we have positive edge weights that corresponds to the minimum priority. And we talked about Dijkstra but we have, maybe, different ways of doing that. We have to select an edge somehow. And then, we relaxed that edge. u, v, w. And you know about the relaxation step. I wo not bother writing it out right now. But it is basically something where you look at the value of d v. And if d v is greater than d u plus the weight, you relax the edge. And you keep doing this. The other thing that you do in the relaxation is to set the predecessor pointers to be correct. And that is part of the relax routine. And you keep doing this until you ca not relax anymore. All right? So that is our generic shortest path algorithm. There are two problems with this algorithm. The first, which we talked about and both of these have to do with the complexity but the first one is that the complexity could be exponential time, even for positive edge weights. And the particular example we talked about was something where you had an exponential number of paths. And if you had a graph that looks like this, then it is possible that a pathological selection of edges is going to make you relax edges an exponential number of times. And in particular, if you have n nodes in this graph, it is plausible that you had end up getting the complexity of order two raised to n over two point OK? So that is one problem. The second problem, which is actually a more obvious problem, is that this algorithm might not even terminate if this actually will not terminate the way it is written if there is a negative weight cycle reachable from the source. All right, so there is two problems. We fixed the first one. In the case of positive edges are non negative edges. We have a neat algorithm that is an efficient algorithm called Dijkstra that we talked about last time that fixed the first part. But we do not know yet how we are going to handle negative cycles in the general case. We know how to handle negative edges in the case of a DAG a directed acyclic graph but not in the general case. OK? So there is this great little skit from Saturday Night Live from the 1980s so way before your time called The Five Minute University. Anybody seen this? All right. Look it up on YouTube. Do not look it up during lecture but afterwards. So the character here is a person by the name of I forget his real name but his fake name is Father Guido Sarducci. All right? So what is this Five Minute University about? Five Minute University, he is selling this notion and he says, look, five years after you graduate you, essentially, are going to remember nothing. OK? I mean, you are not going to remember anything about all the courses you took, et cetera. So why waste your time on a college education or waste money 100,000 on a college education? You know, for 20 I will teach you in five minutes what you are going to remember five years after you graduate. All right? So let is take it to an extreme. Here is a thirty second version up six thousand and six point And this is what I want you to remember five years or ten years or whatever after you graduate. All right? And maybe the ten second version as polynomial time is great. OK? Exponential time is bad. And infinite time gets you fired. OK? So that is all you need to remember. No, that is all you need to remember for the final. This happens, you know, five years after you graduate. So you need to remember a lot more if you want to take your quiz next week and the final exam. But I think that summarized over here. You have a generic shortest path algorithm. And you realize that if you do this wrong you could very easily get into a situation where a polynomial time algorithm, and we know one for Dijkstra, turns into exponential time in the worst case, you know, for a graph like that because you are selecting edges wrongly. And in particular, that is problem number one. And problem number two is if you have a graph that is not what you expect. In this case, let is say you expected that a graph with no negative cycles or maybe not even negative edges in it. You could easily get into a situation where your termination condition is such that your algorithm never completes. So we need to fix problem number two today using this algorithm called Bellman Ford. And as it turns out, this algorithm is incredibly straightforward. I mean, its complexity we will have to look at. But from a description standpoint, it is four lines of code. And let me put that up. So Bellman Ford takes a graph, weights, and a source s. And you can assume an adjacency list specification of the graph or the representation of the graph. And we do some initialization. It is exactly the same as in the generic case except the d values will still be looking at the d values and talking about the relaxation operation. So we do an initialization. And then, this algorithm has multiple passes because for I equals one to v minus one point So it does v minus one passes roughly order v passes where v is the number of vertices. And in each of these passes for each edge u v belonging to e relaxes every edge. And just so everyone remembers, relax u, v, w is if d of v is greater than d of u plus w u v then we will set d v to be and we also set pi v to be u. OK. That is relax operation over here. So that is the algorithm. And if you know magically that they are no negative cycles in the graph. So if they are no negative cycles in the graph, then after these we will have to prove this. But after these v minus one passes you are going to get the correct shortest pathways. OK? You want to do a little bit more, right? I motivated what we want Bellman Ford to do earlier in the lecture. So you can also do a check. So you may not know if they are negative weight cycles or not. But at this point, you can say I am going to do one more pass so the v path the v is the number of vertices over the graph. So for each edge in the graph, if you do one more relaxation and you see that d v is greater than d u plus w u v. So you are not doing a relaxation. You are doing a check to see if you can relax the edge. Then report minus v negative cycle exists. So this is the check. And the first part is the computation. So that is kind of neat. I mean, it fit is on a board. We talk about the correctness. The functionality, I hope everyone got. Do people understand what is happening here with respect to functionality? Any questions? Not about correctness but functionality? Yeah? Where does the get used in the formula? Oh, it does not. It is just a counter that makes sure that you do v minus one passes. So what is that complexity of this algorithm using the best data structure that we can think of? Anyone? Yeah, go ahead. v plus e if you are using a to access? v plus e? Or v e plus e. So that would be? That is using a dictionary? Yeah, I know. v e plus e would be? That is correct but. . Right. But I mean when do v e plus e you can ignore the e. So say you have just v times e. All right. Good. Here you go. So this part here is v times e. And it does not really matter. I mean, you can use an array structure adjacency list. It is not like Dijkstra where we have this neat requirement for a priority queue and there is different ways of implementing the priority queue. This part would be order of v e. And that gives you the overall complexity. This part here is only one pass through the edges. So that is order e, like you said. So the complexities order v e. And this could be large, as I said before in, I think, the first lecture. e is order of v square in a simple graph. So you might end up with a v cubed complexity if you run Bellman Ford. So there is no question that Bellman Ford is, from a practical standpoint, substantially slower than Dijkstra. You can get Dijkstra down to linear complexity. But this would potentially, at least in terms of vertices, be cubic complexity. So when you have a chance, you want to use Dijkstra. And you are forced to use Bellman Ford because you could potentially have negative weight cycles while you are stuck with that. All right? OK, so why does this work? This looks a bit like magic. It turns out we can actually do a fairly straightforward proof of correctness of Bellman Ford. And we are going to do two things. We are going to not only show that if negative weight cycles do not exist that this will correctly compute shorter stats. But we also have to show that it will detect negative weight cycles if they in fact exist. So there is two parts to this. And let is start. So what we have here for this algorithm is that it can guarantee in a graph g equals v E. If it contains no negative weight cycles then after Bellman Ford finishes execution, d v equals delta s v for all v belonging to v. All right? And then there is that. That is the theorem you want to prove. And the second piece of it is corollary that we want to prove. And that has to do with the check. And this says if a value of d of v fails to converge after v minus one passes there exists a negative weight cycle reachable from s. So those are the two things that we need to show. I will probably take a few minutes to do each of these. That theorem is a little more involved. So one of the first things that we have to do in order to prove this theorem is to think about exactly what the shortest path corresponds to in a generic sense. So when we have source vertex s and you have a particular vertex v then there is the picture that we need to keep in mind as we try and prove this theorem. So you have v0, v1, v2, et cetera all the way to vk. This is my vertex v. This is s. So s equals v0. V equals vk. All right? So I am going to have a path p. That is v0, v1, all the way to vk. OK? How big is k in the worst case? How big is k? Anybody? How big is k? It is up on the black board. . v minus 1, right? Why? What would happen if k is larger than v minus 1? I had have a cycle. I had be visiting a vertex more than once. And it would not be a simple path. Right? So k is less than or equal to v minus one else I had have a cycle. OK? I would not have a simple path. And we are looking for the shortest, simple paths because if you ever get to the point where why are we looking for shortest, simple paths? Well, in this case, we are looking for shortest, simple paths. And if there is a negative cycle, we are in trouble because the shortest path is not necessarily the simple path because you could go around the cycle a bunch of times. I will get back to that. But in the case where we are trying to prove the theorem, we know that no negative cycles exist. We can assume that no negative cycles exist for the case of the theorem. And we want to show that Bellman Ford correctly computes each of the shortest path weights. And in that case, there is no negative weight cycles. We are guaranteed that k is less than or equal to v minus one point All right? Everybody buy that? Good. All right. So that is the picture I want you keep in mind. Let is dive in and prove this theorem. And we prove it using induction. So let v be any vertex. And let is say that we are looking at a path. v0, v1, v2, to vk. And like I said, from v0 equals s to vk equals v. And in particular, I am not going to say that this path p is a shortest path with the minimum number of edges. So there may be many shortest paths. And I am going to pick the one that has the minimum number of edges. If there is a unique shortest path, then that is a given. But it may be that I have a path with four edges that has the same weight as another path with three edges. I am going to pick the one that has three edges. OK? So it may not be unique with respect that they are not necessarily unique shortest paths. But I can certainly pick one. And no negative weight cycles implies that p is simple. And that implies that k is less than or equal to v minus 1, which is what I just argued. Now keep in mind that picture over there to the right. And basically, the argument is going to go as follows. Remember that I am going to be relaxing every edge in each pass of the algorithm. OK? There is no choices here. I am going be relaxing every edge in each pass of the algorithm. And essentially, the proof goes as follows. I am going to be moving closer and closer to vk and constructing this shortest path at every pass. So at some point in the first pass, I am going to relax this edge v0, v1. OK? And at that point, thanks to the optimal substructure property, given that this is the shortest path, this has to be a shortest path, as well. Any subset of the shortest path has to be a shortest path. I am going to relax this edge and I am going to get the value of delta from s to v1. And it is going to be this relaxation that is going to get me that value. And after the first pass, I am going to be able to get to v1. After the second pass, I can get to v2. And after k passes, I am going to be able to get to vk. So I am just growing this frontier one node every pass. And that is your induction. And you can write that out. And I will write it out here. But that is basically it. So after one pass through all of the edges e, we have d of v1 to be delta s v1. And the reason for this is because we will relax. We are guaranteed to relax all the edges. And we will relax the edge v0, v1 during this pass. And we ca not find a shorter path than this path because, otherwise we had violate the optimum substructure property. And that means that it is a contradiction that we selected a shortest path in the first place. So can argue that we have delta s v1 after the first pass. And this goes on. I am going to write out this proof because I think it is important for you guys to see the full proof. But you can probably guess the rest at this point. After one pass, that is what you get. After two passes through e we have d v2 equals delta s v2 because in the second pass we are going to relax edge v1, v2. So it a different edge that needs to be relaxed. But that is cool because I am relaxing all the edges. And I am going to be able to grow my frontier. I am going to be able to compute delta s v2 and the end of my second pass and so on and so forth. So after k passes, we have d vk equals delta s vk. And if I run through v minus one passes, which is what I do in the algorithm, all reachable vertices have delta values. All right? That is basically it. Any questions? It is actually a simpler proof than the Dijkstra proof, which I just sketched last time. I will just give you some intuition of the Dijkstra proof. It is probably a little too painful to do in a lecture. But this one is, as you can see, nice and clean and fits on two boards, which is kind of an important criterion here. So good. All right, so that takes care of the theorem. Hopefully you are all on board with the theorem. And one thing that we have not done is talk about the check. So the argument with respect to the corollary bootstraps this particular argument for the theorem. But this requires the insight that if after v minus one passes, if you can find an edge that can be relaxed, well what does that mean? So at this point, let is say that I have done my v minus one passes and we find an edge that can be relaxed. Well, this means that the current shortest path from s to some vertex that is obviously reachable v is not simple once I have relaxed this edge because I have a repeated vertex. So that means it is not simple to have a repeated vertex that is the same as I found a cycle. And it is a negative weight cycle because I was able to relax the edge and reduce the weight after I added a vertex that cost a cycle. All right? So this cycle has to be negative weight. Found a cycle that is negative weight. All right. That is pretty much it. So it is, I guess, a painful algorithm from a standpoint of it is not particularly smart. It is just relaxing all of the edges a certain fixed number of times. And it just works out because you will find these cycles. And if you keep going, it is like this termination condition. What is neat is that I do not have the generic shortest path algorithm up there anymore. But in effect, what you are saying is after a certain number of passes, if you have not finished, you can quit because you have found a negative cycle. So it is very similar to the generic shortest path algorithm. You are not really selecting the edges. You are selecting all of them, in this case. And you are running through a bunch of different passes. All right? So that is it with respect to Bellman Ford. I want to do a couple of special cases and revisit the directed acyclic graph. But stop me here if you have any questions about Bellman Ford. You first and then back there. Yeah? Maybe I am just confused about the definition of a cycle. But if you had, like, a tree, which had a negative weight edge, would not it produce the same situation where you relaxed that edge. But you would have relaxed that edge previously. But it would not be a cycle, right? Yeah, it would not be a cycle. So let is look at that. That is a fine question. Does not that make assumptions about this structure? Well if you had a tree I mean, a tree is a really simple case. But if you had something like this and if you did have a minus one edge here, right we will do a more complicated example. But let is say you had something like this. two three minus one point And what will happen is if this happens to be your s vertex and in the first step you relax all the edges. And this one would get two. And then, depending on the order in which you relaxed, it is quite possible that if you relax this edge first let is say in the first pass the ordering of the relaxation is 1, 2, and three point So the edges are ordered in a certain way each time, and you are going to be relaxing the edges in exactly the same order each time. All right? It does not matter. The beauty of Bellman Ford is that let is say you relax this edge. Initially, this is at infinity. So this is at zero point This is at infinity. This is at infinity. This is at infinity. If you relax this edge, nothing happens. All right? Then you relax, let is say, this edge because that is number two. This gets set to two. You relax this edge because that is three point And this is zero so nothing happens. Of course, this is already at two so nothing would happen. So the end of the first pass, what you have is this is zero point That is two point This is still infinity. That is still infinity. OK? That is going to stay zero because you ca not reach it from s. So we can, sort of, ignore that. And then, of the second pass, what you have is you start with this edge again because that is the ordering. And this two minus one would give this a one point And then you relax this edge or try to relax this edge. Nothing happens. Try to relax this edge. Nothing happens. And at this point, you have one more pass to go because you got four vertices. And in that past, nothing changes again. So that is what you end up with. You end up with two for this and one for that. OK? That makes sense? So the important thing to understand is that you are actually relaxing all of the edges in every pass. And there is a slightly more complicated example than this that is in the notes. And you can take a look at that offline. There is another question in the back. Did you have a question? Someone raised their hand. Yeah? Yes, I am just curious is there a unknown better algorithm that can do the same thing? No, there is no known better algorithm for solving the general case like this. There are a couple of algorithms that assume weights within a certain range. And then there complexities include both v and e, as well as w where w is the dynamic range of the weights. And depending on what w is, you could argue that they have better complexity. But they are kind of incomparable in the sense that they have this extra parameter, which is the dynamic range of the w. OK? Now there is lots of special cases, like I said, and well take a look at the DAG special case in a second where you could imagine doing better but not for the case where you have an arbitrary graph that could have negative cycles in it because it is got negative rate edges. Yeah? In the corollary, does that assume you have a connected graph because, you know, you could have a negative weight edge in a separate part of the graph, which is not reachable from this. Yeah. So you are going to start when you have an undefined weight. Remember your initialization condition. What is affected by s? Initialize is affected by s. The rest of it is not affected by s because you are just relaxing the edges. Initialize is affected by s because d of s starts out being 0, like I put over here, and the rest of them are infinity. So there is an effect of the choice of the starting vertex. And the rest of it follows that you will get an undefined value, or you will find that negative cycle exists based on whether you can reach it from s or not. So if you happen to have s over here, and it is just the one node, and then it has no edges going out of it, this algorithm would just be trivial. But it would not detect any negative cycles that are not reachable from s. That make sense? Yeah. So there is this it is kind of hidden over there. So I am glad you asked that question. But initialize is setting things up. And that is something that affects the rest of the algorithm because d of s is zero and the rest of them are set to infinity. All right? So if there are no other questions, I will move on to the case of the DAG and talk a little bit about shortest paths versus longest paths. And this is somewhat of a preview of a lecture that Eric is going to give a month from now on complexity and the difference between polynomial time and exponential time, though I am not going to go into much depth here. But there is some interesting relationships between the shortest path problem and the longest path problem that I had like to get to. But any other questions on this? OK, so let me ask a question. Suppose I wanted to find longest paths in a graph and let is say that this graph had all positive edge weights. OK. What if I negated all of the edge weights and ran a Bellman Ford? Would I find the longest path in the graph? Do people understand the question? I do not need this. So maybe we can talk about what a longest path means first. So if this was s and this v1, v2, v3, fairly straightforward, you know how to compute shortest paths now. These are all positive. Even easier. The longest path to v3 is of length. Six because I go here, go there, and go there, right? So that is my longest path. OK? And the shortest path to v3 is of length four point So shortest path, longest paths, have these nice duality. What if I said, well, you know, I can solve the longest path problem, as well, given all of what I have learned about shortest paths simply by negating each of these edges and running Bellman Ford. What would happen? Yeah? shortest path branch values, and if you switched to absolute value, it will give you the longest path. So you think it works? Yeah. It will also check the cycles. So the negative cycles will be the longest path cycles that . But I think that is the key question. What will Bellman Ford do when it is run on this? What would it return? . No, what will Bellman Ford return? I am asking. Someone else? What will Bellman Ford return if I ran this? Undefined. Right? Undefined because you got this negative weight cycle here. . Sorry? Oh Let is put another one in there. Oops, sorry. Now I see. You are right. You are right. I am wrong. And why did you say undefined? I was wrong. OK, good. I got company. Thank you. Thank you. Good. Let is take it all over again. All over again. All right. All right, start over. s v1 v2 v3. Yeah, that is a cycle. All right, good. Cycle. So when you actually negate each of these edges, you end up with a negative weight cycle. So it is plausible that you could have a graph like this one where this strategy wo not work because what would happen is Bellman Ford would come back with, essentially, an abort that says I ca not compute shortest paths because they are undefined. All right? Now it turns out it is actually more subtle than that. What we are trying to do in Bellman Ford is, in the case where negative weight cycles do not exist, we report on the shortest simple path. That is the whole notion of the proof. We say that the path has a certain length, which is, at most, v minus one and so on and so forth. We get the shortest simple path. But if you actually have a problem where you say let me start over again. Let is say I want to find the shortest simple path for a different graph and it happens to have a negative weight cycle in it. So I have something like this. two three minus 6, three over here, three over here, and so on. Maybe two here. And I want to find the shortest simple path that reaches v from s. OK? What is the shortest simple path that reaches v from s? It is this path that goes horizontally, which has a weight three plus 2, 5, five plus 3, 8, eight plus 3, 11, eleven plus 2, thirteen point All right? So the shortest simple path is thirteen point Will Bellman Ford give you any information about this path? . No because in. After it does its v minus one passes, v is reachable from s. But you potentially go through a negative weight cycle before you reach v. OK? So it turns out that if you have a graph with negative weight cycles, finding the shortest simple path is an NP hard problem. It is a really hard problem. That is what NP means. No, it means something else that Eric will explain to you in a month or so. But it means that we do not know any algorithm that is better than exponential time to solve this problem. OK? So amazingly, all you have done is taken the shortest path problem and changed it ever so slightly. You said I want to look for the shortest simple path in the general case where I could, potentially, have negative weight cycles in my graph. And when you do that, all bets are off. You are not in the polynomial time complexity domain anymore. At least, not that we know of. And the best that you can do is an exponential time algorithm to find shorter simple paths. And this problem, as it turns out, is equivalent to the longest path problem in the sense that they are both NP hard. If you can solve one, you could solve the other. So to summarize, what happens here simply is that in the case of Bellman Ford running on the original shortest path problem, you are allowed to abort when you detect the fact that there is a negative cycle. So given that you are allowed to abort when there is a negative cycle, you have a polynomial time solution using Bellman Ford that is not necessarily going to give you shortest path weights but will in the case of no negative cycles. All right? But if you ask for more a little bit more you said, you know, it had be great if you could somehow process these negative cycles and tell me that if I had a simple path and I do not go through cycles what would the shortest weight be, it becomes a much more difficult problem. It goes from order of ve complexity to exponential time complexity to the best of our knowledge. So that is what I had like to leave you with. That there is much more to algorithms than just the ones that we are looking at. And we get a little bit of a preview of this so the difference between polynomial time an exponential time later on in the term. 
</body>
</html>