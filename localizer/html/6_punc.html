<html>
<body>
Continuing in the theme of sorting in general, but in particular, binary search trees, which are a kind of way of doing dynamic sorting, if you will, where the elements are coming and going. And at all times, you want to know the sorted order of your elements by storing them in a nice binary search tree. Remember, in general, a binary search tree is a tree. It is binary, and it has the search property. Those three things. This is a rooted binary tree. It has a root. It is binary, so there is a left child and a right child. Some nodes lack a right or left child. Some nodes lack both. Every node has a key. This is the search part. You store key in every node, and you have this BST property, or also called the search property, that every node if you have a node the stores key x, everybody in the left subtree stores a key that is less than or equal to x, and everyone that is in the right subtree stores a key that is greater than or equal to x. So not just the left and right children, but every descendant way down there is smaller than x. Every descendent way down there is greater than x. So when you have a binary search tree like this, if you want to know the sorted order, you do what is called an in order traversal. You look at a node. You recursively visit the left child. Then you print out the root. Then you recursively visit the right child. So in this case, we had go left, left, print eleven point Print twenty point Go right. Go left. Print twenty six point Print twenty nine point Go up. Print forty one point Go right. Print fifty point Print sixty five point Then check that is in sorted order. If you are not familiar with in order traversal, look at the textbook. It is a very simple operation. I am not going to talk about it more here, except we are going to use it. All right, we will get to the topic of today is lecture in a moment, which is balance. What we saw in last lecture and recitation is that these basic binary search trees, where when you insert a node you just walk down the tree to find where that item fits like if you are trying to insert 30, you go left here, go right here, go right here, and say, oh thirty fits here. Let is put thirty there. If you keep doing that, you can do insert. You can do delete. You can do these kinds of searches, which we saw, finding the next larger element or finding the next smaller element, also known as successor and predecessor. These are actually the typical names for those operations. You can solve them in order h time. Anyone remember what h was? The height. Yeah, good. The height of the tree. So h is the height of the BST. What is the height of the tree? . Sorry? . Log n? Log n would be great, but not always. So this is the issue of being balance. So in an ideal world, your tree is going to look something like this. I have drawn this picture probably the most in my academic career. This is a nice, perfectly balanced binary search tree. The height is log n. This would be the balance case. I mean, roughly log n. Let is just put theta to be approximate. But as we saw at the end of last class, you can have a very unbalanced tree, which is just a path. And there the height is n. What is the definition of height? That is actually what I was looking for. Should be six point zero four two material. Yeah? Is it the length of the longest path always going down? Yeah, length of the longest path always going down. So length of the longest path from the root to some leaf. That is right. OK, so this is I highlight this because we are going to be working a lot with height today. All that is happening here, all of the paths are length log n. Here, there is a path of length n. Some of them are shorter, but in fact, the average path is n over two point It is really bad. So this is very unbalanced. I will put very. It is not a very formal term, but that is like the worst case for BSTs. This is good. This does have a formal definition. We call a tree balanced if the height is order log n. So you are storing n keys. If your height is always order log n, we get a constant factor here. Here, it is basically exactly log n, one times log n. It is always going to be at least log n, because if you are storing n things in a binary tree, you need to have height at least log n. So in fact, it will be theta log n if your tree is balanced. And today is goal is to always maintain that your trees are balanced. And we are going to do that using the structure called AVL trees, which I will define in a moment. They are the original way people found to keep trees balanced back in the 60s, but they are still kind of the simplest. There are lots of ways to keep a tree balanced, so I will mention some other balance trees later on. In particular, your textbook covers two other ways to do it. It does not cover AVL trees, so pay attention. One more thing I wanted to define. We talked about the height of the tree, but I had also like to talk about the height of a node in a tree. Can anyone define this for me? Yeah? It is the level that the node is at. The level that the node is at. That is roughly right. I mean, that is right. It is all about, what is the level of a node? Like how many levels of children it has. How many levels of children it has. That is basically right, yeah. The distance from it to the root. Distance from it to the root. That would be the depth. So depth is counting from above. Height is . Yes, longest path from that node to the leaf. Note that is why I wrote this definition actually, to give you a hint. Here I should probably say down to be precise. You are not allowed to go up in these paths.  All right. Sorry. I have got to learn how to throw. All right. So for example, over here I am going to write depths in red. If you are taking notes it is OK. Do not worry. So length off the longest path from it down to a leaf. Well, this is a leaf, so its height is zero point OK. Yeah, I will just leave it at that. It takes zero steps to get from a leaf to a leaf. This guy is not a leaf. It has a child, but it has a path of length one to a leaf. So it is one. This guy has a choice. You could go left and you get a path of length 1, or you could go right and get a path of length two point We take the max, so this guy has height two point This node has height one point This node has height three point How do you compute the height of a node? Anyone? Yeah. Max of the height of the children plus one point Right. You take the max of the height of the children. Here, two and one point Max is two point Add one point You get three point So it is going to always be this is just a formula. The height of the left child maxed with the height of the right child plus one point This is obviously useful for computing. And in particular, in lecture and recitation last time, we saw how to maintain the size of every tree using data structure augmentation. Data structure augmentation. And then we started with a regular vanilla binary search tree, and then we maintained every time we did an operation on the tree, we also updated the size of the subtree rooted at that node, the size field. Here, I want to store a height field, and because I have this nice local rule that tells me how to compute the height of a node using just local information the height of its left child, the height of its right child. Do a constant amount of work here. There is a general theorem. Whenever you have a nice local formula like this for updating your information in terms of your children, then you can maintain it using constant overhead. So we can store the height of every node for free. Why do I care? Because AVL trees are going to use the heights of the nodes. Our goal is to keep the heights small. We do not want this. We want this. So a natural thing to do is store the heights. When they get too big, fix it. So that is what we are going to do. Maybe one more thing to mention over here for convenience. Leaves, for example, have children that are I mean, they have null pointers to their left and right children. You could draw them explicitly like this. Also some nodes just lack a single child. I am going to define the depths of these things to be negative one point This will be convenient later on. Why negative 1? Because then this formula works. You can just think about it. Like leaves, for example, have two children, which are negative one point You take the max. You add one point You get zero point So that just makes things work out. We do not normally draw these in the pictures, but it is convenient that I do not have to do special cases when the left child does not exist and the right child does not exist. You could either do special cases or you could make this definition. Up to you. OK. AVL trees. So the idea with an AVL tree is the following. We had like to keep the height order log n. It is a little harder to think about keeping the height order log n than it is to think about keeping the tree balance, meaning the left and right sides are more or less equal. In this case, we are going to think about them as being more or less equal in height. You could also think about them being more or less equal in subtree size. That would also work. It is a different balanced search tree. Height is kind of the easiest thing to work with. So if we have a node, it has a left subtree. It has a right subtree, which we traditionally draw as triangles. This subtree has a height. We will call it HL for left. By the height of the subtree, I mean the height of its root. And the right subtree has some height, r. I have drawn them as the same, but in general they might be different. And what we would like is that h sub l and h sub r are more or less the same. They differ by at most an additive one point So if I look at h sub l minus h sub r in absolute value, this is at most 1, for every node. So I have some node x. For every node x, I want the left and right subtrees to be almost balanced. Now, I could say differ by at most 0, that the left and right have exactly the same heights. That is difficult, because that really forces you to have exactly the perfect tree. And in fact, it is not even possible for odd n or even n or something. Because at the very end you are going to have one missing child, and then you are unbalanced there. So 0 is just not possible to maintain, but one is almost as good, hopefully. We are going to prove that in a second. And it turns out to be easy to maintain in log n time. So let is prove some stuff. So first claim is that AVL trees are balanced. Balanced, remember, means that the height of them is always order log n. So we are just going to assume for now that we can somehow achieve this property. We want to prove that it implies that the height is at most some constant times log n. We know it is at least log n, but also like it to be not much bigger. So what do you think is the worst case? Say I have n nodes. How could I make the tree as high as possible? Or conversely, if I have a particular height, how could I make it have as few nodes as possible? That had be like the sparsest, the least balanced situation for AVL trees. Yeah? You could have one node on the last level. One node on the last level, yeah, in particular. Little more. What do the other levels look like? That is correct, but I want to know the whole tree. It is hard to explain the tree, but you can explain the core property of the tree. Yeah? . For every node, let is make the right side have a height of one larger than the left side. I think that is worth a cushion. See if I can throw better. Good catch. Better than hitting your eye. So I am going to not prove this formally, but I think if you stare at this long enough it is pretty obvious. Worst case is when there are multiple worst cases, because right and left are symmetric. We do not really care. But let is say that the right subtree has height one more than the left for every node. OK, this is a little tricky to draw. Not even sure I want to try to draw it. But you basically draw it recursively. So, OK, somehow I have figured out this where the height difference here is one point Then I take two copies of it. It is like a fractal. You should know all about fractals by now. Problem set two. And then you just well, that is not quite right. In fact, I need to somehow make this one a little bit taller and then glue these together. Little tricky. Let is not even try to draw the tree. Let is just imagine this is possible. It is possible. And instead, I am going to use mathematics to understand how high that tree is. Or actually, it is a little easier to think about let me get this right. It is so easy that I have to look at my notes to remember what to write. Really, no problem. All right, so I am going to define n sub h is the minimum number of nodes that is possible in an AVL tree of height h. This is sort of the inverse of what we care about, but if we can solve the inverse, we can solve the thing. What we really care about is, for n nodes, how large can the height be? We want to prove that is order log n. But it will be a lot easier to think about the reverse, which is, if I fix the height to be h, what is the fewest nodes that I can pack in? Because for the very unbalanced tree, I have a height of n, and I only need to put n nodes. That would be really bad. What I prefer is a situation like this, where with height h, I have to put in two to the h nodes. That would be perfect balance. Any constant to the h will do. So when you take the inverse, you get a log. OK, we will get to that in a moment. How should we analyze n sub h? I hear something. Yeah? two to the h minus one. Maybe, but I do not think that will quite work out. Any yeah? So you have only one node in the last level, so it would be 12 to the h plus one point That turns out to be approximately correct, but I do not know where you got 12 to the h plus one point It is not exactly correct. I will tell you that, so that your analysis is not right. It is a lot easier. You guys are worried about the last level and actually what the tree looks like, but in fact, all you need is this. All you need is love, yeah. . No, it is not a half. It is a different constant. Yeah? Start with base cases and write a recursive formula. Ah, recursive formula. Good. You said start with base cases. I always forget that part, so it is good that you remember. You should start with the base case, but I am not going to worry about the base case because it wo not matter. Because I know the base case is always going to be n order one is order one point So for algorithms, that is usually all you need for base case, but it is good that you think about it. What I was looking for is recursive formula, aka, recurrence. So can someone tell me maybe even you could tell me a recurrence for n sub h, in terms of n sub smaller h? Yeah? one plus. one plus n sub h minus one point Not quite. Yeah? N sub h minus one plus n sub h minus two point N plus do you want the one plus? I do not think so. You do. It is a collaboration. To combine your two answers, this should be the correct formula. Let me double check. Yes, whew. Good. OK, why? Because the one thing we know is that our tree looks like this. The total height here is h. That is what we are trying to figure out. How many nodes are in this tree of height h? Well, the height is the max of the two directions. So that means that the larger has height h minus 1, because the longest path to a leaf is going to be down this way. What is the height of this? Well, it is one less than the height of this. So it is going to be h minus two point This is where the n sub h minus one plus n sub h minus two come in. But there is also this node. It does not actually make a big difference in this recurrence. This is the exponential part. This is like itty bitty thing. But it matters for the base case is pretty much where it matters. Back to your base case. There is one guy here, plus all the nodes on the left, plus all the nodes on the right. And for whatever reason, I put the left over here and the right over here. And of course, you could reverse this picture. It does not really matter. You get the same formula. That is the point. So this is the recurrence. Now we need to solve it. What we would like is for it to be exponential, because that means there is a lot of nodes in a height h AVL tree. So any suggestions on how we could figure out this recurrence? Does it look like anything you have seen before? Fibonacci. Fibonacci. It is almost Fibonacci. If I hid this plus 1, which you wanted to do, then it would be exactly Fibonacci. Well, that is actually good, because in particular, n sub h is bigger than Fibonacci. If you add one at every single level, the certainly you get something bigger than the base Fibonacci sequence. Now, hopefully you know Fibonacci is exponential. I have an exact formula. If you take the golden ratio to the power h, divide by root 5, and round to the nearest integer, you get exactly the Fibonacci number. Crazy stuff. We do not need to know why that is true. Just take it as fact. And conveniently phi is bigger than one point You do not need to remember what phi is, except it is bigger than one point And so this is an exponential bound. This is good news. So I will tell you it is about 1. 618. And so we get is that if we invert this, this says n sub h is bigger than some phi to the h. This is our n, basically. What we really want to know is how h relates to n, which is just inverting this formula. So we have, on the other hand, the phi to the h divided by root five is less than n. So I got a log base phi on both sides. Seems like a good thing to do. This is actually quite annoying. I have got h minus a tiny little thing. It is less than log base phi of n. And I will tell you that is about one point four four zero times log base two of n, because after all, log base two is what computer scientists care about. So just to put it into perspective. We want it to be theta log base two of n. And here is the bound. The height is always less than one point four four times log n. All we care about is some constant, but this is a pretty good constant. We had like one. There are binary search tress that achieve 1, plus very, very tiny thing, arbitrarily tiny, but this is pretty good. Now, if you do not know Fibonacci numbers, I pull a rabbit out of a hat and I have got this phi to the h. It is kind of magical. There is a much easier way to analyze this recurrence. I will just tell you because it is good to know but not super critical. So we have this recurrence, n sub h. This is the computer scientist way to solve the recurrence. We do not care about the constants. This is the theoretical computer scientist way to solve this recurrence. We do not care about constants. And so we say, aw, this is hard. I have got n sub h minus one and n sub h minus two point So asymmetric. Let is symmetrify. Could I make them both n sub h minus one point Or could I make them both n sub h minus 2? Suggestions? . Minus two is the right way to go because I want to know n sub h is greater than something in order to get a less than down here. By the way, I use that log is monatomic here, but it is, so we are good. So this is going to be greater than one plus two times n sub h minus two point Because if I have a larger height I am going to have more nodes. That is an easy proof by induction. So I can combine these into one term. It is simpler. I can get rid of this one because that only makes things bigger. So I just have this. OK, now I need a base case, but this looks like two the something. What is the something? H over two point So I will just write theta to avoid the base case. two to the h over two point Every two steps of h, I get another factor of two point So when you invert and do the log, this means that h is also less than log base two of n. Log base two because of that. Factor two out here because of that factor two when you take the log. And so the real answer is 1. 44. This is the correct this is the worst case. But it is really easy to prove that it is, at most, two log n. So keep this in mind in case we ask you to analyze variance of AVL trees, like in problem set three. This is the easy way to do it and just get some constant times log n. Clear? All right, so that is AVL trees, why they are balanced. And so if we can achieve this property, that the left and right subtrees have about the same height, we will be done. So how the heck do we maintain that property? Let is go over here. Mobius trees are supposed to support a whole bunch of operations, but in particular, insert and delete. I am just going to worry about insert today. Delete is almost identical. And it is in the code that corresponds to this lecture, so you can take a look at it. Very, very similar. Let is start with insert. Well, it is pretty straightforward. Our algorithm is as follows. We do the simple BST insertion, which we already saw, which is you walk down the tree to find where that key fits. You search for that key. And wherever it is not, you insert a node there, insert a new leaf, and add it in. Now, this will not preserve the AVL property. So the second step is fix the AVL property. And there is a nice concise description of AVL insertion. Of course, how do you do step two is the interesting part. All right, maybe let is start with an example. That could be fun. Hey, look, here is an example. And to match the notes, I am going to do insert twenty three as a first example. OK, I am also going to annotate this tree a little bit. So I said we store the heights, but what I care about is which height is larger, the left or the right. In fact, you could just store that, just store whether it is plus 1, minus 1, or 0, the difference between left and right sides. So I am going to draw that with a little icon, which is a left arrow, a descending left arrow if this is the bigger side. And this is a right arrow. This is even. Left and right are the same. Here, the left is heavier, or higher, I guess. Here it is even. Here it is left. This is AVL, because it is only one heavier wherever I have an arrow. OK, now I insert twenty three point twenty three belongs it is less than 41, greater than 20, less than 29, less than twenty six point So it belongs here. Here is 23, a brand new node. OK, now all the heights change. And it is annoying to draw what the heights are, but I will do it. This one changes to one point This is zero point This changes to two point This changes to three point This changes to four point Anyway, never mind what the heights are. What is bad is, well, this guy is even. This guy is left heavy. This guy is now doubly left heavy. Bad news. OK, let is not worry about above that. Let is just start. The algorithm is going to walk up the tree and say, oh, when do I get something bad? So now I have 23, 26, twenty nine in a path. I had like to fix it. Hmm, how to fix it? I do not think we know how to fix it, so I will tell you how. Actually, I was not here last week. So did we cover rotations? No. OK, good. Then you do not know. Let me tell you about rotations. Super cool. It is just a tool. That is x and y. I always get these mixed up. So this is called left rotate of x. OK, so here is the thing we can do with binary search trees. It is like the only thing you need to know. Because you have got search in binary search trees and you have got rotations. So when I have a tree like this, I have highlighted two nodes, and then there is the children hanging off of them. Some of these might be empty, but they are trees, so we draw them as triangles. If I just do this, which is like changing which is higher, x or y, and whatever the parent of x was becomes the parent of y. And vice versa, in fact. The parent of y was x, and now the parent of x is y. OK, the parent of a is still x. The parent of b changes. It used to be y. Now it is x. The parent of c was y. It is still y. So in a constant number of pointer changes, I can go from this to this. This is constant time. And more importantly, it satisfies the BST order property. If you do an in order traversal of this, you will get a, x, b, y, c. If I do an in order traversal over here, I get a, x, b, y, c. So they are the same. So it still has BST ordering. You can check more formally. b has all the nodes between x and y. Still all the nodes between x and y, and so on. You can check it at home, but this works. We call it a left rotate because the root moves to the left. You can go straight back where you came from. This would be a right rotate of y. OK, it is a reversible operation. It lets you manipulate the tree. So when we have this picture and we are really sad because this looks like a mess, what we had like to do is fix it. This is a path of three nodes. We had really prefer it to look like this. If we could make that transformation, we had be happy. And we can. It is a right rotate of twenty nine point So that is what we are going to do. So let me quickly copy. I want to rotate twenty nine to the right, which means twenty nine and twenty six this is x. This is y. I turn them, and so I get twenty six here now, and twenty nine is the new right child. And then whatever was the left child of x becomes the left child of x in the picture. You can check it. So this used to be the triangle a. And in this case, it is just the node twenty three point And we are happy. Except I did not draw the whole tree. Now we are happy because we have an AVL tree again. Good news. So just check. This is even. This is right heavy. This is even. This is left heavy still. This is left heavy, even, even, even. OK, so now we have an AVL tree and our beauty is restored. I will do one more example. Insert fifty five point We want to insert fifty five here. And what changes is now this is even. This is right heavy. This is doubly left heavy. We are super sad. And then we do not look above that until later. This is more annoying, because you look at this thing, this little path. It is a zigzag path, if you will. If I do a right rotation where this is x and this is y, what I will get is x, y, and then this is b. This is what is in between x and y. And so it will go here. And now it is a zag zig path, which is no better. The height is the same. And we are sad. I told you, though, that somehow rotations are all we need to do. What can I do? How could I fix this little zigzag? Just need to think about those three nodes, but all I give you are rotations. Perhaps rotate fifty point Maybe rotate fifty point That seems like a good idea. Let is try it. If you do not mind, I am just going to write 41, and then there is all the stuff on the left. Now we rotate fifty point So sixty five remains where it is. And we rotate fifty to the left. So fifty and its child. This is x. This is y. And so I get fifty five and I get fifty point Now, this is bad from an AVL perspective. This is still doubly left heavy, this is left heavy, and this is even. But it looks like this case. And so now I can do a right rotation on 65, and I will get so let me order the diagrams here. I do a right rotate on 65, and I will get forty one point And to the right I get fifty five point And to the right I get sixty five point To the left I get fifty point And then I get the left subtree. And so now this is even, even, even. Wow. How high was left subtree? I think it is still left heavy. Cool. This is what some people call double rotation, but I like to call it two rotations. It is whatever you prefer. It is not really a new operation. It is just doing two rotations. So that is an example. Let is do the general case. It is no harder. You might say, oh, gosh, why do you do two examples? Well, because they were different. And they are are two cases on the algorithm. You need to know both of them. OK, so AVL insert. Here we go. Fix AVL property. I am just going to call this from the changed node up. So the one thing that is missing from these examples is that you might have to do more than two rotations. What we did was look at the lowest violation of the AVL property and we fixed it. When we do that, there is still may be violations higher up, because when you add a node, you change the height of this subtree, the height of this subtree, the height of this subtree, and the height of this subtree, potentially. What happened in these cases when I was done, what I did fixed one violation. They were all fixed. But in general, there might be several violations up the tree. So that is what we do. Yeah, I will leave it at that. So suppose x is the lowest node that is not AVL. The way we find that node is we start at the node that we changed. We check if that is OK. We update the heights as we go up using our simple rule. And that is actually not our simple rule, but it is erased. We update the height based on the heights of its children. And you keep walking up until you see, oh, the left is twice, two times or not two times, but plus two larger than the left, or vice versa. Then you say, oh, that is bad. And so we fix it. Yeah, question. So here we continue to. Yes. . add n to the level than one point So add. AVL property is not about levels. It is about left subtrees and right subtrees. So the trouble is that sixty five you have a left subtree, which has height two or sorry, height 1, I guess because the longest path from here to a leaf is one point The right subtree has height negative one because it does not exist. So it is one versus negative one point So that is why there is a double arrow. Yeah, good to ask. It is weird with the negative 1s. That is also why I wanted to define those negative 1s to be there, so the AVL property is easier to state. Other questions? All right. Good. I think I want a symmetry assumption here. I do not know why I wrote right of x. I guess in modern days we write x dot right. Same thing. OK, I am going to assume that the right child is the heavier one like we did before. Could be the left. It is symmetric. It does not matter. So now there are two cases, like I said. I am going to use this term right heavy because it is super convenient. OK, right heavy is what I have been drawing by a descending right arrow. Balance is what I have been drawing by a horizontal line. OK, so we are just distinguishing between these two cases. This turns out to be the easy case. So we have x, y, a, b, c. Why are we looking at the right child? Because we assumed that the right one is higher, so that x was right heavy. So this subtree as I have drawn it is higher than the left one by 2, in fact. And what we do in this case is right rotate of x. And so we get x, y, a, b, c. I could have drawn this no matter what case we are in, so we need to check this actually works. That is the interesting part. And that is over here. OK, so I said x is right heavy, in fact doubly so. y is either right heavy or balanced. Let is start with right heavy. So when we do this rotation, what happens to the heights? Well, it is hard to tell. It is a lot easier to think about what the actual heights are than just these arrows. So let is suppose x has height k. That is pretty generic. And it is right heavy, so that means the y has height k minus one point And then this is right heavy, so this has height k minus two point And this is something smaller then k minus two point In fact, because this is AVL, we assume that x was the lowest that is not AVL. So y is AVL. And so this is going to be k minus 3, and this is going to be k minus three because these differ by two point You can prove by a simple induction you never get more than two out of whack because we are just adding 1, off by one point So we got off by two point So this is the bad situation. Now we can just update the heights over here. So k minus three for a, k minus three for b, k minus two for c. Those do not change because we did not touch those trees, and height is about going down, not up. And so this becomes k minus 2, and this becomes k minus one point And so we changed the height of the root, but now you can see that life is good. This is now balanced between k minus three and k minus three point This is now balanced between k minus two and k minus two point And now the parent of y may be messed up, and that is why after this we go to the parent of y, see if it is messed up, but keep working our way up. But it worked. And in the interest of time, I will not check the case where y is balanced, but it works out, too. And see the notes. So the other case is where we do two rotations. And in general, so here x was doubly right heavy. And the else case is when the right child of x, which I am going to call z here, is left heavy. That is the one remaining situation. You do the same thing, and you check that right rotating and left rotating, which makes the nice picture, which is x, y, z, actually balances everything and you restore the AVL property. So again, check the notes on that. I have a couple minutes left, and instead I had like to tell you a little bit about how this fits into big picture land. Two things I want to talk about. One is you could use this, of course, to sort, which is, if you want to sort n numbers, you insert them and you do in order traversal. How long does this take? In order traversal takes linear time. That is the sense in which we are storing things in sorted order. Inserting n items well, each insert takes h time, but now we are guaranteed that h is order log n. So all the insertions take log n time each, n log n total. So this is yet another way to sort n items in n log n time, in some ways the most powerful way. We have seen heaps, and we have seen merge sort. They all sort. Heaps let you do two operations, insert and delete min, which a lot of times is all you care about, like in p set two. But these guys, AVL trees, let you do insert, delete, and delete min. So they are the same in those senses, but we have the new operation, which is that we can do find next larger and next smaller, aka successor and predecessor. So you can think about what we call an abstract data type. These are the operations that you support, or that you are supposed to support. If you are into Java, you call this an interface. But this is an algorithmic specification of what your data structure is supposed to do. So we have operations like insert and delete. We have operations like find the min and things like successor and predecessor, or next larger, next smaller. You can take any subset of these and it is an abstract data type. Insert, delete, and min is called a priority queue. So if you just take these first two, it is called a priority queue. And there are many priority queues. This is a generic thing that you might want to do. And then the data structure on the other side is how you actually do it. This is the analog of the algorithm. OK, this is the specification. You want a priority queue. One way to do it is a heap. Another way to do it is an AVL tree. You could do it with a sorted array. You could do lots of sub optimal things, too, but in particular, heaps get these two operations. If you want all three, you basically need a balanced binary search tree. There are probably a dozen balanced binary search trees out there, at least a dozen balanced search trees, not all binary. They all achieve log n. So it does not really matter. There are various practical issues, constant factors, things like that. The main reason you prefer a heap is that it is in place. It does not use any extra space. Here, you have got pointers all over the place. You lose a constant factor in space. But from a theoretical standpoint, if you do not care about constant factors, AVL trees are really good because they get everything that we have seen so far and log n. And I will stop there. 
</body>
</html>