<html>
<body>
So we are going to do rolling caches then, we are going to go a little bit over amortized analysis and if we have a lot of time left, we are going to talk about good and bad hash functions. So can someone remind me what is the point of rolling hashes? What is the problem? What are we trying to solve in lectures? Be brave. Gets faster, I think, because like So what are we trying to solve? You do not need to go ahead, tell me what is the big problem that we are trying to solve. I do not remember OK, so let is go over that. So we have a big document, AKA a long string, and we are trying to find a smaller string inside it. And we are trying to do that efficiently. So say the big document is you might have seen this before. And we are trying to look for the here. How do I do that with rolling hashes? So the slow, nice solution is I get this the and then I overlap with the beginning of the document, I do a string comparison. If it matches, I say that it is a match. It is not, I overlap it here. String match, I overlap it here. String match, so on and so forth. The problem is this does a lot of string matching operations, and the string matching operation is how expensive? What is the running time? Order n. Order n, where n is the size of the string. So if we have a string, say this is the key that we are looking for and n is the document size then this is going to be order n times k. We want to get to something better. So how do I do this with rolling hashes? We take the strings up, and you come up with a hash code for it. OK so we are going to hash this. And let is say this is the key hash. OK, very good. And then once you know that, then you will need to compute the next letter hash, or just add it on to that pairing. OK, so next letter for Yeah. So you compute the hash of the entire string, n, capital n Let is not do that. Let is compute the hash of the first key characters in the string. Are we separating them by space? inside?? Yeah, so this is going to be a character. The space will? Sorry? The space will be a character? Yeah. So let is take the first three characters and compute the hash of that. And let is call this the our sliding window. So we are going to say that window has the and then we are going to compute the hash of the characters in the window, and we are going to see that this matches the hash of the key. And then we will figure out what to do. That aside, we are going to slide the window to the right by one character so take out key and put in the space. And now the window has HE space, we are going to compute the hash of the window, see that it is not the same as this hash of the key. What do we know in this case? Different hashes means Not the same string. For sure not the same string. So this is not the. OK, now suppose I am sliding my window so after this I will slide my window again, and I would have e space f. Right, so on and so forth. Suppose I am happy sliding my window and then I get here and I have my window be IN space, and the hash the window happens to match the hash of the key. So we are in the same situation as here. Now what? . Very good. We have to check if the string inside the window is the same as the string inside the key. And if it is, we found a match. If it is not, we keep working. All right? And we have to do the same thing here. So this is our string matching algorithm. Can we somehow make sure that we make a hash function such that it will never Excellent question. Thank you, I like that. Can we make a hash function so that we do not have any false positives, right? Let is see. How do hash functions work? What is the argument to a hash function and what is the return value? The argument is something that you want to hash. So in this case we are working with three character strings. But let is say we are looking for a one megabyte string inside the one gigabyte string. Say we are a music company and we are looking for our mp3 file inside the big files off a pirate server or something. So this is one million character strings, because that is the window size. And it is going to return what? What do hash functions return for them to be useful? Integers. Nice small integers, right? Ideally, the integer would fit in a word size, where the word is the register size on our computer. What are popular word sizes? Does anyone know? . Excellent. 32 bits, 64 bits integers. OK, so what is the universe size for this function? How many one million character strings are there? How many characters are there? Excellent. Let is say we are old school and we are doing? SG?. We do not care about the rest of the world. two hundred and fifty six characters? OK. To the one millionth power. Cool. Let is say we are working on an old school computer, since we are old school and we have a 32 bit word size. How many possible values for the hash function? two to the thirty two point The? other? is bigger. You are messing with me, right? OK so this is two to the 8th. two to the eight million, right? Yup. So this is a lot bigger than this. So if we want to make a hash function that gives us no false positives, then we had have to be able to if we have the universe of possible inputs and the universe of possible outputs, draw a line from every input to a different output. But if we have two to the thirty two by the way is about four billion. If we have four billion outputs and a lot of inputs as we draw our lines, we are eventually going to run out of outputs and we are going to have to use the same output again and again. So for a hash function, pretty much always the universe size is bigger than the output size. So hash functions will always have collisions. So a collision for a hash function is two inputs, x1, x2, so that is x1 is not x2, but h of x1 equals h of x2. So this will always happen. There is no way around it. What we are hoping for is that the collisions are not something dumb. So we are hoping that the hash function acts in a reasonably randomly and we talked about ideal hash functions that would pretty much look like they would get a random output for every input. And we are not going to worry too much about that. What matters is that as long as the hash function is reasonably good, we are not going to have too many false positives. So say the output set is O, so O is two to the thirty two point Then we are hoping to have false positives about one every O times. So one out of two to the thirty two false positives. So what is the running time for when you slide the window and we are doing this logic here. What is the running time if the hashes are not the same? What is the running time of the hash function Of the whole matching algorithm. No, no no, of the hash function itself. Can we make any assumptions about that? Very good. What is the running time of the hash function? So we are going to have to implement if we implement the hash function naively, then the running time for hashing a key character string is order key. But we are going to come up with magic way of doing it in order one time. So assume hashing is order one point What is the running time for everything else? So if the hashes do not match, we know it is not a candidate. So we are going to keep going. So this is order one point What if the hashes do match? characters. Order I mean, but it depends on how many ones match, but it will be So for one match, what is the running time for one match? Order k Order k. Excellent. So the total running time is the number of matches times order k plus the number of non matches times order one point So as long as the number of false positives here is really tiny, the math is going to come out to be roughly order one per character. So the whole thing is order in. Everything should be order n, yeah, that is what we are hoping for. OK so let is talk about the magic because you asked me what is the running time for the hash function and this is the interesting part. How do I get to compute these hashes and order one instead of order k? We have this data structure called rolling hash. So rolling hash does anyone remember from lecture what it is? Is not that what we are doing right now? So this is a sliding window. And the data structure will compute fast hashes for the strings inside the sliding window. So how does it work? I mean not how does it work functionally, what are the operations for a rolling hash, let is try that. Oh. OK, so we have two updates. One of them is pop. For some reason, our notes call it skip, but I like pop better, so I am going to write skip and think pop. And the other one is? Always. A pen with a new character, OK? Cool. So these are the updates. Now what is the point of those updates? What is the query for a rolling hash? . You just grab the next character, append that, and then skip. OK, so this is how I update the rolling hash to contain to reflect the contents of my sliding window. And what I do after that? What is the reason for that? You skip your. So do not think too hard, it is a really easy question. I moved a sliding window here. What do I want to get? You want to get the hash of those characters. The hash of those characters, very good. So this is the query. So a rolling hash has a sequence of characters in it, right? Say t, h, e. And it allows us to append the character and pop a character. Append a character, pop a character. And then it promises that it is going to compute the hash of whatever is inside the rolling hash really fast. Append goes here, skip goes here. How fast do these operations need to be for my algorithm to work correctly? Order one point I promised you that computing hash there is order 1, right? So I have to OK. Let is see how we are going to make this happen. So these are letters. These make sense when we are trying to understand string matching. But now we are going to switch the numbers, because after all, strings are sequences of characters, and characters are numbers. And because I know how to do math on numbers, I do not know how to do math on characters. So let is use this list. Let is say that instead of having numbers in base two hundred and fifty six which is, we are going to have numbers in base 100, because it is really easy to do operations in base one hundred on paper. So 3, 14, 15, 92, 55, 35, 89, 79, thirty one point So these are all base one hundred numbers. And say my rolling window is size five point One, two, three, four, five. So I want to come up with a way so that I have the hash of this and then when I slide my window, I will get the hash of this. What hashing method do we use for rolling hashes? Does anyone remember? . Mod, you said I heard mod something. Yeah, that is what I said. OK, so? So? So the hash is? The hash of a key is? It is k mod m or m k.  OK. I am going to say k mod something, and I am going to say that something has to be a prime number and we will see why in a bit. Let is say our prime number is twenty three point So let is compute the value of the hash for the sliding window of the first sliding window and then we will compute the hash for the second sliding window. Oh, there is some at the computer, sweet. three hundred and fourteen million, one hundred and fifty nine thousand, two hundred and sixty five modulo twenty three is how much? OK, while you are doing that, can someone tell me what computation will he need to do for the second sliding window? one billion, five hundred and nineteen million, two hundred and sixty five thousand, three hundred and fifty nine point one hundred and fifty nine million, two hundred and sixty five thousand, three hundred and fifty nine point That is a third sign. There is a 1 4 before that. The first one is eleven point OK. And what is the second one? adding one billion, four hundred and fifteen million, nine hundred and twenty six thousand, three hundred and thirty five modulo twenty three point five point I heard a five and a seven point OK. Hold on, hold on. I will take the average of those two and we can move on, right? Three five, and arguably six point All right, so let is implement an operation called slide. And slide will take the new number that I am sliding in. And the old number that I am sliding out, making my life really easy. So in this case, the numbers would be The new one is thirty five point And the old one? three point Excellent. And I want to have an internal state called hash that has eleven and I want to get six after I am done running slide. This is still too hard for me, so before we figure out hash, let is say that we have an internal state called n. And n is this big number here. So I want to get from this big number to this big number. What am I going to do? Mod 3,000. OK, so you want to take the big number one hundred and fifty nine thousand, two hundred and sixty five and mod it.  So if I mod it to by a big number, that is going to be too slow. So I ca not mod it. Ca not you just divide it? Division is also slow, I do not like the division. I like subtraction, someone said subtraction. So what I want to do is I want to get from here to a number that to this number, right? So I want to get rid of the three and I want to add thirty five at the end. To get rid of the 3, what do I subtract? three with a bunch of 0s. three with a bunch of 0s. Excellent. 1, 2, 3, 4, 5, 6, 7, eight point How many of them are there? eight point OK, how many digits conveys 100? Oh, two right? four point Oh, oh. four point So base 100, so two numbers One base one hundred number is two digits. Yep. So eight point yeah, OK, four point Cool. So let is try to write this in a more abstract way. So n is the old n minus old, right, so that three is old times what do I have to multiply it by to get all those zeros? k minus 1? to that base whatever. OK, so base to the size to something. K minus one point So K is five in this case, right? My window is five point And I see a four there, so I am going to add the minus one just because that is what I need to do. OK, so then I get fourteen million, one hundred and fifty nine thousand, two hundred and sixty five point What do I do to tack on a thirty five at the end? thirty five point OK, times the base, so that is going to give me the zeroes. And then this is a minus here. And then I am going to add thirty five point Right? one billion, four hundred and fifteen million, nine hundred and twenty six thousand, five hundred and thirty five point Look, it is right. So what do I write here? The base first. Good point. OK. Let me play with this a little bit before we go further. I am going to distribute the base here. So this is n times base minus old times base to the k plus mu. And let is rename base to size to be the size of the window, I do not like k. And I am renaming it because later on we are going to break our slide into appends and skip and the size wo not be constant anymore. OK so does this make sense? It is all math. So this math here becomes abstract math here. But nothing else changes. OK, so now I want to get hash I want to get hash out of n, how do I do that? Mod twenty three point Mod 23, very good. So in a general way, I would say mod p. OK so hash is n times base minus old times base to the size plus new mod p. Now let is distribute this. I know I can distribute modulo across addition and subtraction, so I have n mod p times base minus old times base to the size mod p plus new. And everything still has to be a mod p. So can someone tell me where did I add the mod p? Why did I put it here and here? the original? OK, nmodp is hash, let is do that. So what is true about both n and base to the size? Constant. Constant? Like can you please repeat it? You could base to the size but you ca not hash, I mean Hm. OK, so keep this in mind that we can compute this, because we are going to want to do that later. But what I had in mind is the opposite of constant, because n is huge. Right? And base to the size is also huge, right? N is this number. Base to the size is this number here. one followed by this many zeros, so these numbers are big. All the other numbers are small. Base is small, old is small, new is small, p is small. So I want to get rid of the big numbers, because math with big numbers is slow. So unless I get rid of the big numbers, I am not going to get to order one operation. So we already got rid of this one because it is hash and how do I get rid of this one?  There is some six thousand and forty two algorithm that does that quickly. Well, we definitely just went over this in class today. Which is why you needed the prime number, right? Not quite. There is an algorithm that does it quickly. That algorithm is called repeated squaring and the quickest wait, I am not done, I promise I am not done. So the quickest that this guy can run if you do everything right is order of? log? size. If the window size is one megabyte, ten megabytes, if the window size keeps growing, if the window size is part of the input size, is this constant? Nope. So I ca not do that. Someone else gave me the right answer before. What did you say before? Pre compute it? OK. It is a constant, so why do not we pre compute it? Take it out of here, compute it once, and after that, we can use it all the time. And unless someone has a better name for it, I am going to call this magic. The name has to be short, by the way, because I will be writing this a few times. OK, so now we have hash equals hash times base minus old times magic plus new modulo p. Does not look too bad, right? Pretty constant time. Now let is write the pseudo code for the rolling hash, and let is break this out into an append and a skip at the same time. What if hash is bigger than your word size? So hash is always going to be something modulo p. Oh that is true, OK. So as long as p is decent, it is not going to get too big. All right. What if old and new So old and new P is a big number. three hundred and fourteen million, one hundred and fifty nine thousand, two hundred and sixty nine is possibly bigger than your word size, right? Definitely. So that is why we are getting rid of it. That is true. So this is k digits in base b. Too much. Not going to deal with it. Hash is one digit in base p, because we are doing it mod p. Old and new are one digit base b. So hopefully small numbers. OK, I have not seen a constructor in CLRS, so I am going to say that when you write pseudocode, the method name for a constructor is in it because we have seen this before. And let is say our constructor for a rolling cache starts with the base that we are going to use. And it builds an empty rolling hash, so first there is nothing in it. And then you append and you skip and you can get the hash. What about p? Should not you also do p? Sure. Do that. So let is say base and p are set, so somethings sets base and p. And we need to compute the initial values for hash and magic. What is hash? Zero. There is nothing in there, right? The number is zero point What is magic? . Well, I mean, you can calculate it, right? So magic is based to the size mod p. What size? zero point Just one mod p. Yep. So when I start, I have an empty sliding window. Nothing in there, size is 0, base to the size is 1, whatever the size is. Very good. Let is write append. Hash is? So here, we are doing both an append and the skip. We have to figure out which operation belongs to the append, which operations belong to the skip. So someone help me out. We know subtraction would Multiply mod base. Yup. So this is the append, right? And this is the skip. So hash equals hash. Times base plus new mod p. Very good. This is important. If you do not put this in, Python knows how to deal with big numbers. So it will take your code and it will run it, and you will get the correct output. But hash will keep growing and growing and growing because you are computing n instead of hash. And you will wonder why the code is so slow. So do not forget this. What else do I need to update? OK, I do not have a constant for that, but I have a constant I for something else. Magic. So magic is base to the size mod p. So what happened to the window size? Oh. Times base. Excellent. The window size grows by 1, therefore, I have to multiply this by base. Magic times base mod p. Does p always have to be less then the base, or can it be anything? It can be bigger than the base. So if I want to not have a lot of false positives, then suppose my base is 256, because that is an extra character. I was arguing earlier that the number of false positives that I have is 1P basically. So I want p to be as close to the word size as possible. So p will be around two to the four billion. So definitely bigger. It can work either way. It is better if it is bigger for the algorithm that we are using there. All right, good question, thank you. Skip. Let is implement skip. Hash is? Hash minus old then comes magic . OK, can I write this in Python? What happens if I write this? magic is, We wo not be able to find it. OK so sorry, not in Python. So assume all these are instance variables done the right way, but what happens if old times magic is bigger than hash? I get a negative number. And in math, people assume that if you do something like minus three modulo 23, you are going to get twenty point So modulo is always positive in modular arithmetic, but in a programming language, if you do minus three modulo 20, I am pretty sure you are going to get minus three point And things will go back. So we want to get to a positive number here so that the arithmetic modulo p will work just like in math. So we want to add something to make this whole thing positive. That is something times. OK, so if we are working modulo p then we can add anything to our number, any multiple of p, and the result modulo p does not change. For example, here to get from minus three to 20, I added twenty three point Right? OK, so I want to add a correction factor of p times something. So what should that be? I want to make sure that this whole thing is positive.  So let is see. How big are these guys, by the way? Magic is something mod p, right? So it is definitely smaller or equal to p. How about old?  OK. So smaller or equal than? Base. Base. Very good. So this whole thing is definitely going to be smaller than. So this is definitely going to be smaller than base time p, right? So let is put that in here. You can get fancy and say hey, this is smaller than p, and this is old, so you can put old here instead, same thing. OK so we have hash. Now what do we do to magic? divide it by the base and mod p. It seems base? and p? do not share factors. You are allowed to do that? OK, so skip part two. Magic equals So what if my magic is something like five and my base is 100? How is this going to work? This is where we use fancy math. And I call it fancy math because I did not learn it in high school. So I am assuming at least some of you do not know how this works. So if we are working modulo p, you can think twenty three if you prefer concrete numbers instead. For any number between one and p minus 1, there is something called the multiplicative inverse, a to the minus 1, that also happens to be an integer between one and p minus one point And if you multiply, say, a times b, that is another number. And then you multiply this by a minus 1, you are going to get to b modulo p. So a minus one cancels a in a multiplication. Now let is see if you guys are paying attention. What is a times a to the minus one modulo p? one point OK. Sweet. So suppose I want to find the multiplicative inverse of six point What is it? Is that the mod 23? Yeah. Can someone think of what it should be? four point 4, wow, fast. So six times four equals 24, which is one modulo twenty three point Now let is see if this magic really works, this math magic. So six times seven equals? forty two point Which is what mod 23? Computer guys. Negative 4, so five point Ah, just kidding. Yeah. OK now let is multiply nineteen by four point What is this? seventy six point All right, seventy six modulo 23? seven maybe. Are you kidding? Did you compute it, or did you use sixty nine OK. Started with 7, ended with seven point So this works. So as long we are working modulo a prime number, we can always compute multiplicative inverses. And Python has a function for that, so I will let you Google its standard library to find out what it is. But it can be done, that is what matters as far as we are concerned. So we are going to say that magic is magic times base minus one mod p, which is the multiplicative inverse everything mod p. Now suppose this base minus one modulo p, this multiplicative inverse algorithm is really slow. What do we do to stay order 1? Pre compute it. Base is not going to change. Very good. So the inverse of base, I base, is base minus one mod p. So here I replace this with I base. OK so skip part one is there, skip part two is here. Does this make sense so far? I see some confusion. A lot. A lot to take in at once? Yes. OK. So remember this concept. So this is where we started from. Then we computed n, then after n, we worked modulo p to gets to hashes. So by working module p, we are able to get rid of all the big numbers and we only have small numbers in our rolling hash. And there is that curveball there, there is that inverse, multiplicative inverse, but Python computes it for you, so as long as it is in the initializer, here you do not need to worry about it, because it is not part of the rolling hash operations. By the way, what is the cost of the rolling hash operations? What is the cost of new? Sorry, what is the cost of append? Not thinking here. Constant. All these are small numbers, so the arithmetic is constant, right? What is the cost of skip? Skip part one here, skip part two there. What is the cost of skip? Constant.. All the numbers are small. We went through a lot of effort to get that, so skip is order one point We are missing hash. How would we implement the hash operation? A hash query. It is easy. Sorry? lookup. So a rolling hash has append, skip, and hash. I want to implement that hash function. Hash. We are computing hash all the time. Return. Sorry, I did not understand what you meant by lookup. It is one of our states. Yeah. Exactly. So the hash function returns hash, right? What is the cost of that? Constant. So append is constant time, yes? Skip is constant time. Hash is constant time. We are done. This works. Any questions on rolling hashes before you have to implement one of your own? would not it be easier to use a shift function? Then you do not have to think about plus and minus. A shift function. Well I mean like, you can shift bit wise, right? OK. So you can just use shift instead of thinking about where to add this, where to subtract this. Well so I do bit operations if I am willing to work with these big numbers. But then you have to compute the mod of some big number, right? Like just like that. For this one, you do not have to, because you have the original hash. Oh, you mean the big number being the actual word you are looking at? Yeah. So doing shift is equivalent to maintaining a list and pushing and popping things into the list. And then you have to do a hash, it is equivalent to looking over the entire list and computing the hash function. Because you had have a big number and you have to take it modulo twenty three point And that is order of the size of the big number. But we are not allowed to do that. Hash has to be constant time, otherwise this thing is slow. Why do we compute magic numbers then? Why do we compute magic? We compute magic because somewhere here, we had this base to the size mod p and this could get big. So I ca not afford to keep it around and do math with it all the time. So I ca not compute base to the size every time I want to do append. Would it be worth it if you are computing one hundred different values for matching and, so all you had have to do is, when reassigning magic, just look up So if you do that, then you have to compute values for all the sizes, right? For all the window sizes. Right. So if we assume that window sizes will be less than 100, it does not take very long. Well what if the window size is one million? What if I am looking for a one million character in a one gigabyte string? But would not after all, would not the size just be around the string, like plus or minus the size of the base? So Only if So why would the size change again? Why would not it just be I mean, if you are looking at one character. So if I have a sliding window like this, then it does not change. But if I want to implement a rolling hash, that is a bit more general and that supports append and skip. Whenever I append, the size increases. Whenever I skip, the size decreases. Oh, you are not doing those at every time step. You are doing them as needed. So I am trying to implement that, that can do them in any sequence. Oh. OK. I thought we were just doing sliding window. So if we are just doing sliding window, you can This is really more caterpillar hash instead of rolling hash, like it is more general. Yeah. It is a bit more general. So let is look at rolling hash for the window. And what you are saying is, hey, the window size is constant, so Why do we repeat magic? Yeah, if the window size is constant, then we would not re compute it. It would not change. But with this thing, it is not. OK. But I guess it does not really matter, but even if you call these in the same order, then is not that wasting a lot of computing cycles because just shrinking and then growing every single operation? Oh, well it turns out that a lot of computing cycles is still order one, right? Everything is order one. So as algorithms people, we do not care. If you are doing it in a system and you actually care about that, then OK. But you are still going to have to compute the initial value at some point. But if you know window is staying the same, you do not need to that computation every time? If you sorry? If you know you are actually just doing a window rolling hash Yup. So then you would initialize magic here to be whatever you want it to be, right? But then when you add the first few characters to the window, you have to figure out how to add them. So the code gets more messy. It turns out that this is actually simpler than doing it that way. magic I guess I am just confused because it seems like we are still working with the large numbers every time. Oh. Let is see. Mod p, mod p. That is not so even though you are still multiplying magic times base, it does not matter. After I am going that, I am reducing it modulo p. Yeah. And then because we are only working with the smaller values. Yup. So everything here stays between zero and base or zero and p. Actually hash is between zero and p and magic is between zero and p. OK. How big does p usually get? How big does p usually get. So And let me get back to this. So I was arguing that the number of false positives here is one over O, right? is the number of values that the hash function can output. How many hash functions can we output using a rolling hash? P. P. OK. So the number of false positives is 1P. So what do we want for p? We want p to be the word size, because but if p is the word size, then So p ca not be the word size, because it has to be prime, right? But we want it to be big, because as p becomes bigger, 1P becomes smaller. So there are two constraints. We want p to be big so that we do not have a lot of false positives. And we want p to be small so that operations do not take a lot of time. So in engineering, this is how things work. We call it a tradeoff because there are forces pushing in opposite directions, and it turns out that a reasonable answer to the trade off is you make p fit in a word so that all those operations are still implementable by one CPU instruction. You ca not have it be the word size. So if we are working on a 32 bit computer, I ca not have this be two to the thirty two point But I can have a prime number that is just a little bit smaller than two to the thirty two point Wait, why ca not it be the word size? Or why ca not it be two to the 32? So if p would be this instead of a prime, then I ca not do this. Oh, right right right, yeah I knew that. There are a lot of moving parts here and they are all interconnected. You could do that for any prime number, right? Yup. So this works for prime numbers, but it does not work for non prime numbers. You could find the multiplicative inverse for any prime number in base thirty two point Is that true? I mean any odd number is what I am trying to say. No, that is not true. I refuse to answer hard math questions. They need to be relatively prime. They need to share no factors. Yes, it might be true. So an odd will not share a factor with two to the 32? You are forcing me to remember hard math. Yeah, I totally just thought about this as number. So, no, it turns out that there is no if you are working modulo and non prime base, then there is no multiplicity inverses. So some numbers have no multiplicative inverses, and other numbers have more than one multiplicative inverse. And then the whole thing does not work. So let me see if I can make this work without having an example by hand. Let is say we are working mod 8, right? Mod eight point So two to the minus one mod eight is not going to exist, right? Right, but three will. three point Let is see what do we use? three times three is 9, right? So this is one point How about three times 5? fifteen mod eight is seven point So three and 5, and then eleven point OK. So three times seven would be 21, five point OK so three and three is the multiplicative inverse of itself, and five and seven are yeah. I have to build a more complicated example, but this breaks down in some cases. I will have to get back to you. I will look at my notes for modular arithmetic and I will get back to you guys over email for why and how that breaks. Yes. Sorry, can you tell me again why we did the part two in skip? Like why did we do that? I am not really sure. So we started with magic one and then we in order for this to work, we agree that magic will be base to the size modulo p all the time. So this has to be invariant for my rolling hash. When I do an append, the size increases by one point And then I multiply by base to modulo p. When I do a skip, the size decreases by one point So I have to change magic, because magic is always base times size, so I have to update it. So this is why this happened. Because initially, I wanted to update by dividing it by base, right? Magic divided by base. But if magic is five and base is 100, we are not going to get an integer. And we want to stay within integers, so that is when I pulled out fancy math and OK. OK. So how are we doing with rolling hashes? Good? All this math will be in the notes, right? Everything. Oh, yeah, everything else will be in the notes. Before we close out, I want to show you one cute thing. Who remembers amortized analysis? I know there is one person that said they understood. All The growing, shrinking thing is what we did in lecture. I want to show something else. I want to show you a binary tree. A binary search tree, because you have seen this on the PSAT and you already hate it. Why had they call it amortization? Because I looked it up online, it means to kill, and so I am like, why not say like, attrition or something else that is a little bit less Amortization is also used in accounting to mean you are INTERPOSING VOICES Let is use the growing hash example, because that is good for why this is the case. So when you are growing your table, you are inserting. If you still have space, that is order one. If not, you have to grow your table to insert. And that is more expensive. That is order n where n is how many elements you had before. So if you graph this costs, if you start off with a table of size one, you can insert the first element for a cost of one. For the second element, you have to resize the table, so it is a cost of two. Now when you are trying to insert the third element, you have to resize the table again to a size of four point But when you insert the fourth element, it is free. Well, cost of one. When you insert the fifth element, you have to resize a table to the size of eight, right? So The table size is one, two, four, four, and now it is eight. But because they resized this to eight, the next three assertions are going to be order one. And then the one after that is going to make the table be sixteen point So I can do seven insertions for free and then I am going to have to pay a lot more for the next one. Someone said dampening. I like dampening because the idea behind amortization is that you can take you have these big costs and they do not occur very often. So you can think of it as taking these big costs and chopping them up. For example, I am going to chop this up into four and I am going to take this piece and put it here. This piece and put it here. This piece and put it here. And then I am going to chop this guy into two, and then take this piece and put it here. And the beginning is a little bit weird, let is not worry about that but this guy, if I chop this guy up into eight, it is going to happen, is it? Well we can put so this guy grows exponentially, right? Every time it is multiplied by two point But the gap size here also is multiplied by two point So when I chop this up and I re distribute the pieces, it turns out that the pieces are the same size. So if I apply a dampening function that does this, then the costs are going to look they are not going to be on one, they there are going to be three or something. And they look like this. Now, my CPU time is going to look like this, right? That is not going to change, because that is what is really happening. But what I can argue for is that if I look for a bunch of operations, say if I look at the first sixteen insertions, the cost of those is the sum of these guys. So it is not been squared, which is what you would get if you look at the worst cases here, but it is order n. So this is what is being dampened, the amount of time an operation takes. Does this make some sense? All right, I want to show you a cute example for amortization. And I will try to make it quick. So how do you list the keys in a binary search tree in order?  In order traversal, right? OK, there is another way of doing it that makes perfect intuitive sense. Get the minimum key, right? And then output the minimum key, then while you can get the next largest, which is the successor so while this is not, now output that key, right? If you do the thing within order traversal, you get order end running time. What is the running time for this? For n. You are going through all the keys, too. Yeah, but next largest what is the running time for next largest? Log n. So this guy is log in, right? So I have n keys, so this whole thing is O of n logn. So it is definitely not bigger than n logn. But now, let is look at what happens using the tree. When I call min, I go down on each edge. And then I call successor and it outputs this guy. Then I call successor and it goes here. Than I call successor and it goes up here and here and outputs this guy. Successor goes here. Successor goes here. Successor goes all the way down here, successor goes up here, successor goes here, and then successor goes all the way up to the roots and gives up.  So how many times do I traverse each edge? Exactly twice, right? How many edges in the tree? If I have n nodes, how many lines do I use to connect them?  So one node, zero lines. two nodes, one line. Three nodes, two lines. So n nodes, n minus one. N asymptotically, good. Good answer. Order, n, edges. Right, each edge gets traversed exactly twice. So amortized cost for n next largest operations is order n. So you can do this instead. This code makes a lot more sense than in order traversal. OK, and the last part is remember that list query that was on the PSAT? Turns out you can do a find for the lowest element and then call successor until you see the highest element for the same argument. Well, I could not tell you this for the PSAT because we had not learned amortized analysis, so you would not be able to prove that your code is fast. But now if you get the intuition, you can write it that way. And your code will still be fast. Same running time. So the intuition for that is a bit more complicated. The proof is more complicated. But the intuition is that say this is l and this is h. Then I am going to go in this tree here. So the same edge magic is going to happen, except there will be logn edges that are unmatched here and logn edges that are not unmatched here. Because once I find the node that is next to h, I will stop, right? So some edges will not be matched. So then I will say that the total running time is logn plus a. i being the number of elements you pull out, right? Yup. So this is amortized analysis. The list is hard. The traversal is easy. Remember the traversal. That is easy to reason about, so that is good. OK. Any questions on amortized analysis? So the idea is that you look at all the operations, you do not look at one operation at a time. And you are trying to see if I look at everything, is it the case that I have some really fast operations and the slow operations do not happen too much, because if that is the case, then I can make an argument for the average cost, which is better than the argument that says this is the worst case of an operation, I am doing an operation, the total cost is n times the worst cost. Make some sense? OK. Cool. All right. Have fun at the next p set. 
</body>
</html>